{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc40d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Train a YOLOv5 model on a custom dataset\n",
    "\n",
    "Usage:\n",
    "    $ python path/to/train.py --data coco128.yaml --weights yolov5s.pt --img 640\n",
    "\"\"\"\n",
    "from train_with_bcc import VOL_ID_MAP, SINGLE_VOL_ID_MAP\n",
    "from train_with_bcc import convert_target_volunteers_yolo2bcc\n",
    "from train_with_bcc import get_file_volunteers_dict\n",
    "from label_converter import BACKGROUND_CLASS_ID\n",
    "from timeit import default_timer as timer\n",
    "from collections import defaultdict\n",
    "import pdb\n",
    "import argparse\n",
    "from label_converter import qt2yolo_optimized, qt2yolo\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "from label_filter import filter_qt\n",
    "\n",
    "from PIL.ImageFont import truetype\n",
    "from label_converter import yolo2bcc_new, find_union_cstargets, targetize, yolo2bcc_newer\n",
    "from train_with_bcc import convert_yolo2bcc, nn_predict, convert_cs_yolo2bcc\n",
    "from train_with_bcc import read_crowdsourced_labels, init_bcc_params, \\\n",
    "    init_nn_output, compute_param_confusion_matrices, init_metrics, update_bcc_metrics\n",
    "from lib.BCCNet.VariationalInference.VB_iteration_yolo import VB_iteration as VBi_yolo\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FILE = Path(__file__).absolute()\n",
    "# sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path\n",
    "\n",
    "import val  # for end-of-epoch mAP\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Model\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
    "    strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\n",
    "    check_requirements, print_mutation, set_logging, one_cycle, colorstr, methods\n",
    "from utils.downloads import attempt_download\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.plots import plot_labels, plot_evolve\n",
    "from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, intersect_dicts, select_device, \\\n",
    "    torch_distributed_zero_first\n",
    "from utils.loggers.wandb.wandb_utils import check_wandb_resume\n",
    "from utils.metrics import fitness\n",
    "from utils.loggers import Loggers\n",
    "from utils.callbacks import Callbacks\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n",
    "RANK = int(os.getenv('RANK', -1))\n",
    "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6caefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_opt(known=False):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--bcc_epoch', type=int, default=0, help='start-epoch for BCC+YOLO run; use -1 for no BCC.')\n",
    "    parser.add_argument('--qtfilter_epoch', type=int, default=-1, help='start-epoch for qt-filter; use -1 for no qt-filter.')\n",
    "    parser.add_argument('--qt_thres_mode', type=str, default='', help=\"one of '', 'conf-count', 'entropy', 'conf-val'\")\n",
    "    parser.add_argument('--qt_thres', type=float, default=0.0, help=\"the threshold value.\")\n",
    "    parser.add_argument('--hybrid_entropy_thres', type=float, default=0.0, help=\"the entropy threshold value (only to be used when running the hybrid filter).\")\n",
    "    parser.add_argument('--hybrid_conf_thres', type=float, default=0.0, help=\"the confidence threshold value (only to be used when running the hybrid filter).\")\n",
    "    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')\n",
    "    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n",
    "    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='dataset.yaml path')\n",
    "    parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch.yaml', help='hyperparameters path')\n",
    "    parser.add_argument('--epochs', type=int, default=300)\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\n",
    "    parser.add_argument('--rect', action='store_true', help='rectangular training')\n",
    "    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')\n",
    "    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n",
    "    parser.add_argument('--noval', action='store_true', help='only validate final epoch')\n",
    "    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n",
    "    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')\n",
    "    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n",
    "    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in \"ram\" (default) or \"disk\"')\n",
    "    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\n",
    "    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')\n",
    "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\n",
    "    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\n",
    "    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')\n",
    "    parser.add_argument('--project', default='runs/train', help='save to project/name')\n",
    "    parser.add_argument('--entity', default=None, help='W&B entity')\n",
    "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--quad', action='store_true', help='quad dataloader')\n",
    "    parser.add_argument('--linear-lr', action='store_true', help='linear LR')\n",
    "    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')\n",
    "    parser.add_argument('--upload_dataset', action='store_true', help='Upload dataset as W&B artifact table')\n",
    "    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval for W&B')\n",
    "    parser.add_argument('--save_period', type=int, default=-1, help='Log model after every \"save_period\" epoch')\n",
    "    parser.add_argument('--artifact_alias', type=str, default=\"latest\", help='version of dataset artifact to be used')\n",
    "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\n",
    "    parser.add_argument('--freeze', type=int, default=0, help='Number of layers to freeze. backbone=10, all=24')\n",
    "    parser.add_argument('--patience', type=int, default=1100, help='EarlyStopping patience (epochs)')\n",
    "    opt = parser.parse_known_args()[0] if known else parser.parse_args('')\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab75535",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ec8153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 8c57633 torch 1.9.0 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mbcc_epoch=0, qtfilter_epoch=-1, qt_thres_mode=, qt_thres=0.0, hybrid_entropy_thres=0.0, hybrid_conf_thres=0.0, weights=yolov5s.pt, cfg=, data=data/single_toy_bcc.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=10, batch_size=20, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=0, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=1100\n"
     ]
    }
   ],
   "source": [
    "data_name = 'single_toy_bcc'\n",
    "opt = parse_opt()\n",
    "opt.data = f'data/{data_name}.yaml' # Does not make any difference!\n",
    "data_dict_path = f'../../datasets/{data_name}'\n",
    "opt.exist_ok = False\n",
    "opt.cache = None\n",
    "opt.workers = 0\n",
    "opt.batch_size = 20 # Change this to number of train images\n",
    "opt.epochs = 10\n",
    "opt.bcc_epoch = 0\n",
    "\n",
    "# Checks\n",
    "set_logging(RANK)\n",
    "if RANK in [-1, 0]:\n",
    "    print(colorstr('train: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\n",
    "#     check_git_status()\n",
    "#     check_requirements(requirements=FILE.parent / 'requirements.txt', exclude=['thop'])\n",
    "\n",
    "# Resume\n",
    "if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # resume an interrupted run\n",
    "    ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\n",
    "    assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\n",
    "    with open(Path(ckpt).parent.parent / 'opt.yaml') as f:\n",
    "        opt = argparse.Namespace(**yaml.safe_load(f))  # replace\n",
    "    opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate\n",
    "    LOGGER.info(f'Resuming training from {ckpt}')\n",
    "else:\n",
    "    opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check files\n",
    "    assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\n",
    "    if opt.evolve:\n",
    "        opt.project = 'runs/evolve'\n",
    "        opt.exist_ok = opt.resume\n",
    "    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))\n",
    "\n",
    "# DDP mode\n",
    "device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "if LOCAL_RANK != -1:\n",
    "    from datetime import timedelta\n",
    "    assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'\n",
    "    assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'\n",
    "    assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'\n",
    "    assert not opt.evolve, '--evolve argument is not compatible with DDP training'\n",
    "    torch.cuda.set_device(LOCAL_RANK)\n",
    "    device = torch.device('cuda', LOCAL_RANK)\n",
    "    dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\")\n",
    "hyp = opt.hyp\n",
    "callbacks=Callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a9801",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74cfc3e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=10.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-sharma\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">light-hill-90</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/g-sharma/YOLOv5\" target=\"_blank\">https://wandb.ai/g-sharma/YOLOv5</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/g-sharma/YOLOv5/runs/zhcwpkpu\" target=\"_blank\">https://wandb.ai/g-sharma/YOLOv5/runs/zhcwpkpu</a><br/>\n",
       "                Run data is saved locally in <code>/Users/gs0029/repos/dental_disease/yolobcc/wandb/run-20211011_134125-zhcwpkpu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7066239 parameters, 7066239 gradients, 16.4 GFLOPs\n",
      "\n",
      "Transferred 356/362 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.00046875\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'datasets/single_toy_bcc/labels/train.cache' images and labels... 8 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'datasets/single_toy_bcc/labels/train.cache' images and labels... 8 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'datasets/single_toy_bcc/labels/val.cache' images and labels... 2 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp44\u001b[0m\n",
      "Starting training for 10 epochs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.96, Best Possible Recall (BPR) = 1.0000\n"
     ]
    }
   ],
   "source": [
    "torchMode = True\n",
    "save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \\\n",
    "    Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n",
    "    opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n",
    "vol_id_map = SINGLE_VOL_ID_MAP if 'single' in data else VOL_ID_MAP\n",
    "\n",
    "bcc_epoch = opt.bcc_epoch\n",
    "qtfilter_epoch = opt.qtfilter_epoch\n",
    "qt_thres_mode = opt.qt_thres_mode\n",
    "qt_thres = opt.qt_thres\n",
    "hybrid_entropy_thres = opt.hybrid_entropy_thres\n",
    "hybrid_conf_thres = opt.hybrid_conf_thres\n",
    "# if not opt.bcc:\n",
    "#     bcc_epoch = -1\n",
    "# Directories\n",
    "w = save_dir / 'weights'  # weights dir\n",
    "w.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "last, best = w / 'last.pt', w / 'best.pt'\n",
    "\n",
    "# Hyperparameters\n",
    "if isinstance(hyp, str):\n",
    "    with open(hyp) as f:\n",
    "        hyp = yaml.safe_load(f)  # load hyps dict\n",
    "LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\n",
    "\n",
    "# Save run settings\n",
    "with open(save_dir / 'hyp.yaml', 'w') as f:\n",
    "    yaml.safe_dump(hyp, f, sort_keys=False)\n",
    "with open(save_dir / 'opt.yaml', 'w') as f:\n",
    "    yaml.safe_dump(vars(opt), f, sort_keys=False)\n",
    "data_dict = {\n",
    "    'path': data_dict_path,  # dataset root dir\n",
    "    'train': 'images/train',  # train images (relative to 'path') 128 images\n",
    "    'val': 'images/val',  # val images (relative to 'path') 128 images\n",
    "    'test': 'images/test', # test images (optional)\n",
    "    'nc': 2,  # number of classes\n",
    "    'names': ['bone-loss', 'dental-caries']  # class names\n",
    "}\n",
    "for x in ['train', 'val', 'test']:\n",
    "    data_dict[x] = os.path.join(data_dict['path'], data_dict[x])\n",
    "\n",
    "# Loggers\n",
    "if RANK in [-1, 0]:\n",
    "    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n",
    "    if loggers.wandb:\n",
    "        data_dict = loggers.wandb.data_dict\n",
    "        if resume:\n",
    "            weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp\n",
    "\n",
    "    # Register actions\n",
    "    for k in methods(loggers):\n",
    "        callbacks.register_action(k, callback=getattr(loggers, k))\n",
    "\n",
    "# Config\n",
    "plots = not evolve  # create plots\n",
    "cuda = device.type != 'cpu'\n",
    "init_seeds(1 + RANK)\n",
    "with torch_distributed_zero_first(RANK):\n",
    "    data_dict = data_dict or check_dataset(data)  # check if None\n",
    "train_path, val_path = data_dict['train'], data_dict['val']\n",
    "nc = 1 if single_cls else int(data_dict['nc'])  # number of classes\n",
    "names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
    "assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check\n",
    "is_coco = data.endswith('coco.yaml') and nc == 80  # COCO dataset\n",
    "\n",
    "# Model\n",
    "pretrained = weights.endswith('.pt')\n",
    "if pretrained:\n",
    "    with torch_distributed_zero_first(RANK):\n",
    "        weights = attempt_download(weights)  # download if not found locally\n",
    "    ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "    model = Model(cfg or ckpt['model'].yaml, ch=1, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "    exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys\n",
    "    csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "    csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n",
    "    model.load_state_dict(csd, strict=False)  # load\n",
    "    LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report\n",
    "else:\n",
    "    model = Model(cfg, ch=1, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "\n",
    "# Freeze\n",
    "freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = True  # train all layers\n",
    "    if any(x in k for x in freeze):\n",
    "        print(f'freezing {k}')\n",
    "        v.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "nbs = 64  # nominal batch size\n",
    "accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
    "hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
    "LOGGER.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\n",
    "\n",
    "g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "for v in model.modules():\n",
    "    if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\n",
    "        g2.append(v.bias)\n",
    "    if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\n",
    "        g0.append(v.weight)\n",
    "    elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n",
    "        g1.append(v.weight)\n",
    "\n",
    "if opt.adam:\n",
    "    optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "else:\n",
    "    optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "\n",
    "optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "optimizer.add_param_group({'params': g2})  # add g2 (biases)\n",
    "LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups \"\n",
    "            f\"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias\")\n",
    "del g0, g1, g2\n",
    "\n",
    "# Scheduler\n",
    "if opt.linear_lr:\n",
    "    lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\n",
    "else:\n",
    "    lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
    "\n",
    "# EMA\n",
    "ema = ModelEMA(model) if RANK in [-1, 0] else None\n",
    "\n",
    "# Resume\n",
    "start_epoch, best_fitness = 0, 0.0\n",
    "if pretrained:\n",
    "    # Optimizer\n",
    "    if ckpt['optimizer'] is not None:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        best_fitness = ckpt['best_fitness']\n",
    "\n",
    "    # EMA\n",
    "    if ema and ckpt.get('ema'):\n",
    "        ema.ema.load_state_dict(ckpt['ema'].float().state_dict())\n",
    "        ema.updates = ckpt['updates']\n",
    "\n",
    "    # Epochs\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    if resume:\n",
    "        assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'\n",
    "    if epochs < start_epoch:\n",
    "        LOGGER.info(f\"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.\")\n",
    "        epochs += ckpt['epoch']  # finetune additional epochs\n",
    "\n",
    "    del ckpt, csd\n",
    "\n",
    "# Image sizes\n",
    "gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
    "imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
    "\n",
    "# DP mode\n",
    "if cuda and RANK == -1 and torch.cuda.device_count() > 1:\n",
    "    logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'\n",
    "                    'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# SyncBatchNorm\n",
    "if opt.sync_bn and cuda and RANK != -1:\n",
    "    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "    LOGGER.info('Using SyncBatchNorm()')\n",
    "\n",
    "# Trainloader\n",
    "train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
    "                                          hyp=hyp, augment=False, cache=opt.cache, rect=opt.rect, rank=RANK,\n",
    "                                          workers=workers, image_weights=opt.image_weights, quad=opt.quad,\n",
    "                                          prefix=colorstr('train: '))\n",
    "if bcc_epoch != -1:\n",
    "    file_volunteers_dict = get_file_volunteers_dict(data_dict)\n",
    "\n",
    "n_grid_choices, n_anchor_choices = model.model[-1].nl, model.model[-1].na\n",
    "grid_ratios = model.model[-1].stride.cpu().detach().numpy() / imgsz\n",
    "# if bcc_epoch != -1:\n",
    "#     cstargets_all = read_crowdsourced_labels(data)\n",
    "#     cstargets_union = find_union_cstargets(cstargets_all['train'])\n",
    "#     cstargets_all_bcc = convert_cs_yolo2bcc(cstargets_all, n_anchor_choices, nc, grid_ratios)\n",
    "#     cstargets_bcc = torch.tensor(cstargets_all_bcc['train']) if torchMode else cstargets_all_bcc['train']\n",
    "#     print(\"*** GPU Usage after reading crowdsourced labels\")\n",
    "#     gpu_usage()\n",
    "mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class\n",
    "nb = len(train_loader)  # number of batches\n",
    "assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'\n",
    "\n",
    "# Process 0\n",
    "if RANK in [-1, 0]:\n",
    "    val_loader, val_dataset = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,\n",
    "                                   hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,\n",
    "                                   workers=workers, pad=0.5,\n",
    "                                   prefix=colorstr('val: '))\n",
    "\n",
    "    if not resume:\n",
    "        labels = np.concatenate(dataset.labels, 0)\n",
    "        if plots:\n",
    "            plot_labels(labels, names, save_dir)\n",
    "\n",
    "        # Anchors\n",
    "        if not opt.noautoanchor:\n",
    "            check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
    "        model.half().float()  # pre-reduce anchor precision\n",
    "\n",
    "    callbacks.on_pretrain_routine_end()\n",
    "\n",
    "# DDP mode\n",
    "if cuda and RANK != -1:\n",
    "    model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\n",
    "\n",
    "# Model parameters\n",
    "hyp['box'] *= 3. / nl  # scale to layers\n",
    "hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layers\n",
    "hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layers\n",
    "hyp['label_smoothing'] = opt.label_smoothing\n",
    "model.nc = nc  # attach number of classes to model\n",
    "model.hyp = hyp  # attach hyperparameters to model\n",
    "model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n",
    "model.names = names\n",
    "\n",
    "# Start training\n",
    "t0 = time.time()\n",
    "nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "last_opt_step = -1\n",
    "maps = np.zeros(nc)  # mAP per class\n",
    "results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "scaler = amp.GradScaler(enabled=cuda)\n",
    "stopper = EarlyStopping(patience=opt.patience)\n",
    "compute_loss = ComputeLoss(model)  # init loss class\n",
    "if bcc_epoch != -1:\n",
    "    bcc_params = init_bcc_params(K=len(vol_id_map))\n",
    "    bcc_params['n_epoch'] = epochs\n",
    "    batch_pcm = {k: torch.tensor(v).to(device) if torchMode else v for k, v in compute_param_confusion_matrices(bcc_params).items()}\n",
    "    # pred0_bcc = init_nn_output(dataset.n, grid_ratios, n_anchor_choices, bcc_params)\n",
    "    bcc_metrics = init_metrics(bcc_params['n_epoch'])\n",
    "LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\\n'\n",
    "            f'Using {train_loader.num_workers} dataloader workers\\n'\n",
    "            f\"Logging results to {colorstr('bold', save_dir)}\\n\"\n",
    "            f'Starting training for {epochs} epochs...')\n",
    "\n",
    "times = defaultdict(float)\n",
    "epoch_times = {epoch: defaultdict(float) for epoch in range(start_epoch, epochs)}\n",
    "batch_times = {i: defaultdict(float) for i in range(1 + np.max(dataset.batch))}\n",
    "epoch_batch_times = {epoch: {i: defaultdict(float) for i in batch_times} for epoch in epoch_times}\n",
    "old_lb = float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ca6125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "\n",
      "YOLO+BCC\n",
      "  0%|          | 0/1 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum probs (c1, c2, bkgd): [2.2e-05, 1e-06, 0.855003]\n",
      "Maximum probs (c1, c2, bkgd): [0.110918, 0.042098, 0.999963]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA = {}\n",
    "for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "    epoch_key = f'epoch_{epoch}'\n",
    "    DATA[epoch_key] = {}\n",
    "    bcc_flag = False if bcc_epoch == -1 else (epoch - start_epoch >= bcc_epoch)\n",
    "    qtfilter_flag = False if qtfilter_epoch == -1 else (epoch - start_epoch >= qtfilter_epoch)\n",
    "    if not bcc_flag and qtfilter_flag:\n",
    "        qtfilter_flag = False\n",
    "    LBs = []\n",
    "    model.train()\n",
    "    # Update image weights (optional, single-GPU only)\n",
    "    if opt.image_weights:\n",
    "        cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\n",
    "        iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\n",
    "        dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\n",
    "\n",
    "    # Update mosaic border (optional)\n",
    "\n",
    "    mloss = torch.zeros(3, device=device)  # mean losses\n",
    "    if RANK != -1:\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "    pbar = enumerate(train_loader)\n",
    "    LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "    LOGGER.info('\\n' + ('YOLO+BCC' if bcc_flag else 'Only YOLO'))\n",
    "    if RANK in [-1, 0]:\n",
    "        pbar = tqdm(pbar, total=nb)  # progress bar\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "        batch_key = f'batch_{i}'\n",
    "        DATA[epoch_key][batch_key] = {}\n",
    "        DATA[epoch_key][batch_key]['imgs_unn'] = imgs.cpu().detach().numpy()\n",
    "        DATA[epoch_key][batch_key]['targets'] = targets.cpu().detach().numpy()\n",
    "        if bcc_epoch != -1:\n",
    "            batch_filenames = [dataset.label_files[x].split(os.sep)[-1] for x in np.where(dataset.batch==i)[0]]\n",
    "            batch_volunteers_list = [file_volunteers_dict[fn] for fn in batch_filenames]\n",
    "            batch_volunteers = torch.cat(batch_volunteers_list)\n",
    "            target_volunteers = torch.cat([targets, batch_volunteers.unsqueeze(-1)], axis=1)\n",
    "            batch_size = np.where(dataset.batch==i)[0].shape[0]\n",
    "            target_volunteers_bcc, vigcwh = convert_target_volunteers_yolo2bcc(target_volunteers, n_anchor_choices, nc, grid_ratios, batch_size, vol_id_map)\n",
    "            # batch_cstargets_bcc = (cstargets_bcc[dataset.batch == i]).to(device)\n",
    "            DATA[epoch_key][batch_key]['target_volunteers_bcc'] = target_volunteers_bcc.cpu().detach().numpy()\n",
    "            DATA[epoch_key][batch_key]['batch_volunteers'] = batch_volunteers.cpu().detach().numpy()\n",
    "            DATA[epoch_key][batch_key]['target_volunteers'] = target_volunteers.cpu().detach().numpy()\n",
    "        ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "        DATA[epoch_key][batch_key]['imgs_unn'] = imgs.cpu().detach().numpy()\n",
    "        DATA[epoch_key][batch_key]['targets'] = targets.cpu().detach().numpy()\n",
    "        imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "        # Warmup\n",
    "        if ni <= nw:\n",
    "            xi = [0, nw]  # x interp\n",
    "            accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "            for j, x in enumerate(optimizer.param_groups):\n",
    "                # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                if 'momentum' in x:\n",
    "                    x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "        # Multi-scale\n",
    "        if opt.multi_scale:\n",
    "            sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "            sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "            if sf != 1:\n",
    "                ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Forward\n",
    "        with amp.autocast(enabled=cuda):\n",
    "            pred = model(imgs)\n",
    "            DATA[epoch_key][batch_key]['pred'] = [temp.cpu().detach().numpy() for temp in pred]\n",
    "            if bcc_flag:\n",
    "                model.eval()\n",
    "                batch_pred_yolo = nn_predict(model, imgs, imgsz, transform_format_flag=False) # y_hat_yolo\n",
    "                DATA[epoch_key][batch_key]['batch_pred_yolo'] = batch_pred_yolo.cpu().detach().numpy()\n",
    "                \n",
    "                batch_pred_bcc, _, batch_conf = yolo2bcc_newer(batch_pred_yolo, imgsz, silent=False) # y_hat_bcc\n",
    "                DATA[epoch_key][batch_key]['batch_pred_bcc'] = batch_pred_bcc.cpu().detach().numpy()\n",
    "                break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8712be5a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 25200, 1]),\n",
       " tensor([[[2.],\n",
       "          [2.],\n",
       "          [2.],\n",
       "          ...,\n",
       "          [2.],\n",
       "          [2.],\n",
       "          [2.]],\n",
       " \n",
       "         [[2.],\n",
       "          [2.],\n",
       "          [2.],\n",
       "          ...,\n",
       "          [2.],\n",
       "          [2.],\n",
       "          [2.]],\n",
       " \n",
       "         [[2.],\n",
       "          [2.],\n",
       "          [2.],\n",
       "          ...,\n",
       "          [2.],\n",
       "          [2.],\n",
       "          [2.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[2.],\n",
       "          [2.],\n",
       "          [2.],\n",
       "          ...,\n",
       "          [2.],\n",
       "          [2.],\n",
       "          [2.]],\n",
       " \n",
       "         [[2.],\n",
       "          [2.],\n",
       "          [2.],\n",
       "          ...,\n",
       "          [2.],\n",
       "          [2.],\n",
       "          [2.]],\n",
       " \n",
       "         [[2.],\n",
       "          [2.],\n",
       "          [2.],\n",
       "          ...,\n",
       "          [2.],\n",
       "          [2.],\n",
       "          [2.]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_volunteers_bcc.shape, target_volunteers_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e2bd59",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 25200, 3]),\n",
       " tensor([[[-6.62740e+00, -9.44792e+00, -1.40342e-03],\n",
       "          [-6.39468e+00, -9.05749e+00, -1.78855e-03],\n",
       "          [-6.57640e+00, -9.03238e+00, -1.51349e-03],\n",
       "          ...,\n",
       "          [-5.01375e+00, -4.65666e+00, -1.62758e-02],\n",
       "          [-4.91624e+00, -4.62422e+00, -1.72866e-02],\n",
       "          [-4.80842e+00, -4.54875e+00, -1.89190e-02]],\n",
       " \n",
       "         [[-6.94112e+00, -9.83048e+00, -1.02149e-03],\n",
       "          [-6.69457e+00, -9.38194e+00, -1.32273e-03],\n",
       "          [-6.74522e+00, -9.36138e+00, -1.26328e-03],\n",
       "          ...,\n",
       "          [-5.29581e+00, -4.78025e+00, -1.34971e-02],\n",
       "          [-5.18950e+00, -4.74927e+00, -1.43351e-02],\n",
       "          [-5.04816e+00, -4.74585e+00, -1.52241e-02]],\n",
       " \n",
       "         [[-6.56580e+00, -9.35718e+00, -1.49517e-03],\n",
       "          [-6.32801e+00, -8.91176e+00, -1.92225e-03],\n",
       "          [-6.69840e+00, -9.10357e+00, -1.34505e-03],\n",
       "          ...,\n",
       "          [-4.83213e+00, -4.53226e+00, -1.89034e-02],\n",
       "          [-4.90226e+00, -4.66089e+00, -1.70321e-02],\n",
       "          [-4.86877e+00, -4.67586e+00, -1.71465e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-7.06673e+00, -9.63353e+00, -9.18930e-04],\n",
       "          [-6.73402e+00, -9.21336e+00, -1.29026e-03],\n",
       "          [-6.93342e+00, -9.40090e+00, -1.05789e-03],\n",
       "          ...,\n",
       "          [-4.93510e+00, -4.63951e+00, -1.69958e-02],\n",
       "          [-4.99522e+00, -4.80618e+00, -1.50621e-02],\n",
       "          [-4.92952e+00, -4.76496e+00, -1.58785e-02]],\n",
       " \n",
       "         [[-6.77733e+00, -9.50265e+00, -1.21471e-03],\n",
       "          [-6.48439e+00, -9.02732e+00, -1.64853e-03],\n",
       "          [-6.63989e+00, -9.05645e+00, -1.42485e-03],\n",
       "          ...,\n",
       "          [-4.86945e+00, -4.61736e+00, -1.77124e-02],\n",
       "          [-4.88626e+00, -4.66437e+00, -1.71205e-02],\n",
       "          [-4.82763e+00, -4.61083e+00, -1.81121e-02]],\n",
       " \n",
       "         [[-6.95181e+00, -9.84092e+00, -1.01063e-03],\n",
       "          [-6.72006e+00, -9.40728e+00, -1.28942e-03],\n",
       "          [-6.83369e+00, -9.35226e+00, -1.16434e-03],\n",
       "          ...,\n",
       "          [-4.79179e+00, -4.57199e+00, -1.88108e-02],\n",
       "          [-4.85334e+00, -4.60627e+00, -1.79514e-02],\n",
       "          [-4.89652e+00, -4.63367e+00, -1.73411e-02]]], grad_fn=<LogBackward>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_pred_bcc.shape, batch_pred_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cbc6a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pcm = {k: torch.tensor(v).to(device) if torchMode else v for k, v in compute_param_confusion_matrices(bcc_params).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "65a4ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "YOLOBCC_DATA, YOLO_DATA, YOLO_PT_BCC_DATA = pickle.load(open('SANITY_CHECK_DATA.pkl', 'rb'))\n",
    "df1 = pd.DataFrame({k: v['batch_0'] for k, v in YOLOBCC_DATA.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7bce539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "[[    -1.3853     -1.5387     -1.5387]\n",
      " [    -1.5387     -1.3853     -1.5387]\n",
      " [    -1.5387     -1.5387     -1.3853]]\n",
      "epoch 1\n",
      "[[    -5.3794     -6.2458  -0.0089234]\n",
      " [    -5.1648     -5.6741   -0.012872]\n",
      " [    -6.9778     -8.4032  -0.0011621]]\n",
      "epoch 2\n",
      "[[    -4.4167     -5.3571   -0.019113]\n",
      " [    -3.9437     -4.5774   -0.033485]\n",
      " [    -7.0211     -8.4867  -0.0011046]]\n",
      "epoch 3\n",
      "[[    -3.6724      -4.762    -0.03681]\n",
      " [    -2.9799     -3.8368   -0.078623]\n",
      " [    -7.1128      -8.622  -0.0010001]]\n",
      "epoch 4\n",
      "[[    -3.1644     -4.3784   -0.058677]\n",
      " [    -2.3052     -3.3756     -0.1475]\n",
      " [    -7.2637     -8.7969 -0.00085703]]\n",
      "epoch 5\n",
      "[[    -2.8589      -4.132   -0.078652]\n",
      " [    -1.8757     -3.0996      -0.225]\n",
      " [    -7.4461     -8.9758 -0.00071539]]\n",
      "epoch 6\n",
      "[[     -2.688     -3.9652   -0.093536]\n",
      " [    -1.6049     -2.9286     -0.2978]\n",
      " [    -7.6224     -9.1343 -0.00060242]]\n",
      "epoch 7\n",
      "[[    -2.5996     -3.8466    -0.10319]\n",
      " [    -1.4275     -2.8175    -0.36075]\n",
      " [    -7.7741     -9.2667 -0.00052009]]\n",
      "epoch 8\n",
      "[[    -2.5615     -3.7589    -0.10864]\n",
      " [    -1.3056     -2.7423    -0.41342]\n",
      " [    -7.8972     -9.3747  -0.0004617]]\n",
      "epoch 9\n",
      "[[    -2.5555     -3.6923    -0.11103]\n",
      " [    -1.2185     -2.6901    -0.45689]\n",
      " [    -7.9944      -9.462 -0.00042014]]\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    batch_pred_bcc = df1.loc['batch_pred_bcc', f'epoch_{e}']\n",
    "    batch_pcm_var = df1.loc['batch_pcm_var', f'epoch_{e}']\n",
    "    X, nn_output, alpha_volunteers, alpha0_volunteers, torchMode, device, invert_classes = target_volunteers_bcc, batch_pred_bcc, batch_pcm['variational'], batch_pcm['prior'], torchMode, device, False\n",
    "    ElogPi_volunteer = expected_log_Dirichlet_parameters(alpha_volunteers, torchMode, device=device)\n",
    "    batch_pcm['variational'] = torch.from_numpy(batch_pcm_var)\n",
    "#     print(batch_pcm_var.detach().numpy.)\n",
    "    print('epoch', e)\n",
    "    print(ElogPi_volunteer.detach().numpy().reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f11c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(10):\n",
    "    target_volunteers_bcc, batch_pred_bcc, batch_pcm['variational'], batch_pcm['prior'], torchMode, device, False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f45c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.38528],\n",
       "         [-1.53874],\n",
       "         [-1.53874]],\n",
       "\n",
       "        [[-1.53874],\n",
       "         [-1.38528],\n",
       "         [-1.53874]],\n",
       "\n",
       "        [[-1.53874],\n",
       "         [-1.53874],\n",
       "         [-1.38528]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.BCCNet.VariationalInference.VB_iteration_yolo import expected_log_Dirichlet_parameters \n",
    "X, nn_output, alpha_volunteers, alpha0_volunteers, torchMode, device, invert_classes = target_volunteers_bcc, batch_pred_bcc, batch_pcm['variational'], batch_pcm['prior'], torchMode, device, False\n",
    "\n",
    "ElogPi_volunteer = expected_log_Dirichlet_parameters(alpha_volunteers, torchMode, device=device)\n",
    "ElogPi_volunteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad249a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    -1.3853     -1.5387     -1.5387]\n",
      " [    -1.5387     -1.3853     -1.5387]\n",
      " [    -1.5387     -1.5387     -1.3853]]\n"
     ]
    }
   ],
   "source": [
    "ElogPi_volunteer.shape,\n",
    "print(ElogPi_volunteer.detach().numpy().reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cc6e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.BCCNet.VariationalInference.VB_iteration_yolo import get_modules\n",
    "\n",
    "X, nn_output, ElogPi_volunteer, torchMode, device = X, nn_output, ElogPi_volunteer, torchMode, device\n",
    "\n",
    "# def expected_true_labels(X, nn_output, ElogPi_volunteer, torchMode=False, device=None):\n",
    "base_lib, copy_fn, simple_transpose, maxwithdim_fn, maximum_fn = get_modules(torchMode, ['base_lib', 'copy', 'simple_transpose', 'maxwithdim', 'maximum'])\n",
    "I, U, K = X.shape  # I = no. of image, U = no. of anc hor boxes in total, K = no. of volunteers\n",
    "M = ElogPi_volunteer.shape[0]  # M = Number of classes\n",
    "N = ElogPi_volunteer.shape[1]  # N = Number of classes used by volunteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e0a3e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 25200, 1, 3, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I, U, K, M, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76724a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = copy_fn(nn_output)  # I x U x M logits\n",
    "# eq. 12:\n",
    "# for k in range(K):\n",
    "k = 0\n",
    "inds = tuple([x.long() for x in base_lib.where(X[:, :, k] > -1)])  # rule out missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad827b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.62740)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcba3eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.16615], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ElogPi_volunteer[0, 2]+nn_output[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fde27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho[inds[0], inds[1], :] += simple_transpose(\n",
    "    ElogPi_volunteer[:, base_lib.squeeze(X[inds[0], inds[1], k]).long(), k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7793f216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        ...,\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528]], dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_transpose(ElogPi_volunteer[:, base_lib.squeeze(X[inds[0], inds[1], k]).long(), k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "faaebbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.16615)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "994fbc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 1]), tensor([-1.53874], dtype=torch.float64))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ElogPi_volunteer.shape, ElogPi_volunteer[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0555bb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.53874, -1.53874, -1.53874,  ..., -1.53874, -1.53874, -1.53874],\n",
       "        [-1.53874, -1.53874, -1.53874,  ..., -1.53874, -1.53874, -1.53874],\n",
       "        [-1.38528, -1.38528, -1.38528,  ..., -1.38528, -1.38528, -1.38528]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ElogPi_volunteer[:, base_lib.squeeze(X[inds[0], inds[1], k]).long(), k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dea1614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.62740, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_output[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc89133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        ...,\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528],\n",
       "        [-1.53874, -1.53874, -1.38528]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_transpose(\n",
    "    ElogPi_volunteer[:, base_lib.squeeze(X[inds[0], inds[1], k]).long(), k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "094f5560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([201600]), torch.Size([201600])],\n",
       " (tensor([0, 0, 0,  ..., 7, 7, 7]),\n",
       "  tensor([    0,     1,     2,  ..., 25197, 25198, 25199])))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in inds], inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e27639fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation: (minus the max of each anchor)\n",
    "rho -= simple_transpose(base_lib.tile(simple_transpose(maxwithdim_fn(rho, 2)), (M, 1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1761b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([22215,  2985]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(rho[0, :, :2].detach().numpy(), axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd0e5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # eq. 11:\n",
    "q_t = base_lib.exp(rho) / maximum_fn(1e-60, simple_transpose(base_lib.tile(simple_transpose(base_lib.sum(base_lib.exp(rho), 2)), (M, 1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e069fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([22215,  2985]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(q_t[0, :, :2].detach().numpy(), axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e927db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_t = maximum_fn(1e-60, q_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad61fe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.13552e-03, 6.76486e-05, 9.98797e-01],\n",
       "         [1.43314e-03, 9.99641e-05, 9.98467e-01],\n",
       "         [1.19496e-03, 1.02502e-04, 9.98703e-01],\n",
       "         ...,\n",
       "         [5.71356e-03, 8.16566e-03, 9.86121e-01],\n",
       "         [6.29968e-03, 8.43609e-03, 9.85264e-01],\n",
       "         [7.01849e-03, 9.09943e-03, 9.83882e-01]],\n",
       "\n",
       "        [[8.29706e-04, 4.61416e-05, 9.99124e-01],\n",
       "         [1.06174e-03, 7.22621e-05, 9.98866e-01],\n",
       "         [1.00930e-03, 7.37621e-05, 9.98917e-01],\n",
       "         ...,\n",
       "         [4.30767e-03, 7.21348e-03, 9.88479e-01],\n",
       "         [4.79139e-03, 7.44138e-03, 9.87767e-01],\n",
       "         [5.51947e-03, 7.46774e-03, 9.87013e-01]],\n",
       "\n",
       "        [[1.20768e-03, 7.40754e-05, 9.98718e-01],\n",
       "         [1.53198e-03, 1.15650e-04, 9.98352e-01],\n",
       "         [1.05769e-03, 9.54563e-05, 9.98847e-01],\n",
       "         ...,\n",
       "         [6.85401e-03, 9.25069e-03, 9.83895e-01],\n",
       "         [6.38811e-03, 8.13204e-03, 9.85480e-01],\n",
       "         [6.60578e-03, 8.01132e-03, 9.85383e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[7.31760e-04, 5.61849e-05, 9.99212e-01],\n",
       "         [1.02066e-03, 8.55303e-05, 9.98894e-01],\n",
       "         [8.36123e-04, 7.09022e-05, 9.99093e-01],\n",
       "         ...,\n",
       "         [6.18172e-03, 8.30775e-03, 9.85510e-01],\n",
       "         [5.81941e-03, 7.03037e-03, 9.87150e-01],\n",
       "         [6.21532e-03, 7.32706e-03, 9.86458e-01]],\n",
       "\n",
       "        [[9.77399e-04, 6.40444e-05, 9.98959e-01],\n",
       "         [1.31015e-03, 1.03024e-04, 9.98587e-01],\n",
       "         [1.12143e-03, 1.00063e-04, 9.98779e-01],\n",
       "         ...,\n",
       "         [6.60184e-03, 8.49467e-03, 9.84903e-01],\n",
       "         [6.49122e-03, 8.10385e-03, 9.85405e-01],\n",
       "         [6.88418e-03, 8.55076e-03, 9.84565e-01]],\n",
       "\n",
       "        [[8.20888e-04, 4.56624e-05, 9.99134e-01],\n",
       "         [1.03502e-03, 7.04536e-05, 9.98895e-01],\n",
       "         [9.23827e-04, 7.44373e-05, 9.99002e-01],\n",
       "         ...,\n",
       "         [7.13608e-03, 8.89026e-03, 9.83974e-01],\n",
       "         [6.70923e-03, 8.58964e-03, 9.84701e-01],\n",
       "         [6.42519e-03, 8.35681e-03, 9.85218e-01]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e70a1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([22215,  2985]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(q_t[0, :, :2].detach().numpy(), axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dea5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e0cbce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00987, 0.02214, 0.96798],\n",
       "        [0.01175, 0.01669, 0.97156],\n",
       "        [0.01596, 0.02041, 0.96364],\n",
       "        [0.01020, 0.01977, 0.97003]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_t[0, [25016, 25043, 25046, 25051], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98015fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial of eq. 8: (right side 2nd term)\n",
    "f_iu = base_lib.zeros((M, N, K), dtype=base_lib.float64)\n",
    "if torchMode:\n",
    "    f_iu = f_iu.to(device)\n",
    "for k in range(K):\n",
    "    for n in range(N):\n",
    "        ids0 = base_lib.where(X[:, :, k] == n)[0]\n",
    "        ids1 = base_lib.where(X[:, :, k] == n)[1]\n",
    "        f_iu[:, n, k] = base_lib.sum(q_t[ids0, ids1, :], 0)\n",
    "rho.shape, rho\n",
    "#     return q_t, f_iu, rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35356839",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {}\n",
    "for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "    epoch_key = f'epoch_{epoch}'\n",
    "    DATA[epoch_key] = {}\n",
    "    bcc_flag = False if bcc_epoch == -1 else (epoch - start_epoch >= bcc_epoch)\n",
    "    qtfilter_flag = False if qtfilter_epoch == -1 else (epoch - start_epoch >= qtfilter_epoch)\n",
    "    if not bcc_flag and qtfilter_flag:\n",
    "        qtfilter_flag = False\n",
    "    LBs = []\n",
    "    model.train()\n",
    "    # Update image weights (optional, single-GPU only)\n",
    "    if opt.image_weights:\n",
    "        cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\n",
    "        iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\n",
    "        dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\n",
    "\n",
    "    # Update mosaic border (optional)\n",
    "\n",
    "    mloss = torch.zeros(3, device=device)  # mean losses\n",
    "    if RANK != -1:\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "    pbar = enumerate(train_loader)\n",
    "    LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "    LOGGER.info('\\n' + ('YOLO+BCC' if bcc_flag else 'Only YOLO'))\n",
    "    if RANK in [-1, 0]:\n",
    "        pbar = tqdm(pbar, total=nb)  # progress bar\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "        batch_key = f'batch_{i}'\n",
    "        DATA[epoch_key][batch_key] = {}\n",
    "        DATA[epoch_key][batch_key]['imgs_unn'] = imgs.cpu().detach().numpy()\n",
    "        DATA[epoch_key][batch_key]['targets'] = targets.cpu().detach().numpy()\n",
    "        if bcc_epoch != -1:\n",
    "            batch_filenames = [dataset.label_files[x].split(os.sep)[-1] for x in np.where(dataset.batch==i)[0]]\n",
    "            batch_volunteers_list = [file_volunteers_dict[fn] for fn in batch_filenames]\n",
    "            batch_volunteers = torch.cat(batch_volunteers_list)\n",
    "            target_volunteers = torch.cat([targets, batch_volunteers.unsqueeze(-1)], axis=1)\n",
    "            batch_size = np.where(dataset.batch==i)[0].shape[0]\n",
    "            target_volunteers_bcc, vigcwh = convert_target_volunteers_yolo2bcc(target_volunteers, n_anchor_choices, nc, grid_ratios, batch_size, vol_id_map)\n",
    "            # batch_cstargets_bcc = (cstargets_bcc[dataset.batch == i]).to(device)\n",
    "            DATA[epoch_key][batch_key]['target_volunteers_bcc'] = target_volunteers_bcc.cpu().detach().numpy()\n",
    "            DATA[epoch_key][batch_key]['batch_volunteers'] = batch_volunteers.cpu().detach().numpy()\n",
    "            DATA[epoch_key][batch_key]['target_volunteers'] = target_volunteers.cpu().detach().numpy()\n",
    "        ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "        DATA[epoch_key][batch_key]['imgs_unn'] = imgs.cpu().detach().numpy()\n",
    "        DATA[epoch_key][batch_key]['targets'] = targets.cpu().detach().numpy()\n",
    "        imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "        # Warmup\n",
    "        if ni <= nw:\n",
    "            xi = [0, nw]  # x interp\n",
    "            accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "            for j, x in enumerate(optimizer.param_groups):\n",
    "                # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                if 'momentum' in x:\n",
    "                    x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "        # Multi-scale\n",
    "        if opt.multi_scale:\n",
    "            sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "            sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "            if sf != 1:\n",
    "                ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Forward\n",
    "        with amp.autocast(enabled=cuda):\n",
    "            pred = model(imgs)\n",
    "            DATA[epoch_key][batch_key]['pred'] = [temp.cpu().detach().numpy() for temp in pred]\n",
    "            if bcc_flag:\n",
    "                model.eval()\n",
    "                batch_pred_yolo = nn_predict(model, imgs, imgsz, transform_format_flag=False) # y_hat_yolo\n",
    "                DATA[epoch_key][batch_key]['batch_pred_yolo'] = batch_pred_yolo.cpu().detach().numpy()\n",
    "                \n",
    "                batch_pred_bcc, _, batch_conf = yolo2bcc_newer(batch_pred_yolo, imgsz, silent=False) # y_hat_bcc\n",
    "                DATA[epoch_key][batch_key]['batch_pred_bcc'] = batch_pred_bcc.cpu().detach().numpy()\n",
    "                \n",
    "                batch_qtargets, batch_pcm['variational'], batch_lb = VBi_yolo(target_volunteers_bcc, batch_pred_bcc, batch_pcm['variational'], batch_pcm['prior'], torchMode = torchMode, device=device)\n",
    "                DATA[epoch_key][batch_key]['batch_qtargets'] = batch_qtargets.cpu().detach().numpy()\n",
    "                DATA[epoch_key][batch_key]['batch_pcm_var'] = batch_pcm['variational'].cpu().detach().numpy()\n",
    "                DATA[epoch_key][batch_key]['batch_lb'] = batch_lb.cpu().detach().numpy()\n",
    "\n",
    "                LBs.append(batch_lb)\n",
    "                with torch.no_grad():\n",
    "                    batch_qtargets_yolo = qt2yolo_optimized(batch_qtargets, grid_ratios, n_anchor_choices, vigcwh, torchMode = torchMode, device=device).half().float()\n",
    "                    DATA[epoch_key][batch_key]['batch_qtargets_yolo_full'] = batch_qtargets_yolo.cpu().detach().numpy()\n",
    "                    batch_qtargets_yolo = batch_qtargets_yolo[batch_qtargets_yolo[:,1] != BACKGROUND_CLASS_ID, :]\n",
    "                    DATA[epoch_key][batch_key]['batch_qtargets_yolo'] = batch_qtargets_yolo.cpu().detach().numpy()\n",
    "                model.train()\n",
    "                loss, loss_items = compute_loss(pred, batch_qtargets_yolo)\n",
    "                DATA[epoch_key][batch_key]['loss'] = loss.cpu().detach().numpy()\n",
    "                DATA[epoch_key][batch_key]['loss_items'] = loss_items.cpu().detach().numpy()\n",
    "            else:\n",
    "                loss, loss_items = compute_loss(pred, targets.to(device))\n",
    "                DATA[epoch_key][batch_key]['loss'] = loss.cpu().detach().numpy()\n",
    "                DATA[epoch_key][batch_key]['loss_items'] = loss_items.cpu().detach().numpy()\n",
    "            if RANK != -1:\n",
    "                loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
    "                DATA[epoch_key][batch_key]['loss*WORLD_SIZE'] = loss.cpu().detach().numpy()\n",
    "            if opt.quad:\n",
    "                loss *= 4.\n",
    "                DATA[epoch_key][batch_key]['loss*4'] = loss.cpu().detach().numpy()\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Optimize\n",
    "        if ni - last_opt_step >= accumulate:\n",
    "            scaler.step(optimizer)  # optimizer.step\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            if ema:\n",
    "                ema.update(model)\n",
    "            last_opt_step = ni\n",
    "\n",
    "        # Log\n",
    "        if RANK in [-1, 0]:\n",
    "            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "            DATA[epoch_key][batch_key]['mloss'] = mloss.cpu().detach().numpy()\n",
    "            mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\n",
    "            pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (\n",
    "                f'{epoch}/{epochs - 1}', mem, *mloss, (targets).shape[0], imgs.shape[-1]))\n",
    "            callbacks.on_train_batch_end(ni, model, imgs, (targets), paths, plots, opt.sync_bn)\n",
    "        # end batch ------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Scheduler\n",
    "    lr = [x['lr'] for x in optimizer.param_groups]  # for loggers\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    if RANK in [-1, 0]:\n",
    "        # mAP\n",
    "        callbacks.on_train_epoch_end(epoch=epoch)\n",
    "        ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\n",
    "        final_epoch = epoch + 1 == epochs\n",
    "        if not noval or final_epoch:  # Calculate mAP\n",
    "            results, maps, _ = val.run(data_dict,\n",
    "                                       batch_size=batch_size // WORLD_SIZE * 2,\n",
    "                                       imgsz=imgsz,\n",
    "                                       model=ema.ema,\n",
    "                                       single_cls=single_cls,\n",
    "                                       dataloader=val_loader,\n",
    "                                       save_dir=save_dir,\n",
    "                                       save_json=is_coco and final_epoch,\n",
    "                                       verbose=nc < 50 and final_epoch,\n",
    "                                       plots=plots and final_epoch,\n",
    "                                       callbacks=callbacks,\n",
    "                                       compute_loss=compute_loss)\n",
    "            # yhat_train = pred\n",
    "            # y_train = dataset.labels\n",
    "            # yhat_test = results\n",
    "            # y_test = val_dataset.labels\n",
    "            # update_bcc_metrics(bcc_metrics, qtargets, yhat_train, y_train, yhat_test, y_test, epoch)\n",
    "            DATA[epoch_key][batch_key]['results'] = results\n",
    "        # Update best mAP\n",
    "        fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\n",
    "        if fi > best_fitness:\n",
    "            best_fitness = fi\n",
    "        log_vals = list(mloss) + list(results) + lr\n",
    "        callbacks.on_fit_epoch_end(log_vals, epoch, best_fitness, fi)\n",
    "\n",
    "        # Save model\n",
    "        if (not nosave) or (final_epoch and not evolve):  # if save\n",
    "            ckpt = {'epoch': epoch,\n",
    "                    'best_fitness': best_fitness,\n",
    "                    'model': deepcopy(de_parallel(model)).half(),\n",
    "                    'ema': deepcopy(ema.ema).half(),\n",
    "                    'updates': ema.updates,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None}\n",
    "\n",
    "            # Save last, best and delete\n",
    "            torch.save(ckpt, last)\n",
    "            if best_fitness == fi:\n",
    "                torch.save(ckpt, best)\n",
    "            del ckpt\n",
    "            callbacks.on_model_save(last, epoch, final_epoch, best_fitness, fi)\n",
    "\n",
    "        # Stop Single-GPU\n",
    "        if stopper(epoch=epoch, fitness=fi):\n",
    "            break\n",
    "    try:\n",
    "        lb = LBs[-1]\n",
    "        if epoch > start_epoch and torch.abs((lb - old_lb) / old_lb) < bcc_params['convergence_threshold']:\n",
    "            print('Convergence reached!')\n",
    "            break\n",
    "        old_lb = lb\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        del batch_pred_yolo, batch_pred_bcc, batch_pred_yolo_wh, batch_qtargets, batch_qtargets_yolo\n",
    "    except (UnboundLocalError, NameError):\n",
    "        pass\n",
    "\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "    # end epoch ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# end training -----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2750f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bcc_epoch == -1:\n",
    "    YOLO_DATA = DATA\n",
    "elif bcc_epoch == 0:\n",
    "    YOLOBCC_DATA = DATA\n",
    "else:\n",
    "    YOLO_PT_BCC_DATA = DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0704c689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(YOLO_PT_BCC_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd28f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump((YOLOBCC_DATA, YOLO_DATA, YOLO_PT_BCC_DATA), open('SANITY_CHECK_DATA.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
