{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc40d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Train a YOLOv5 model on a custom dataset\n",
    "\n",
    "Usage:\n",
    "    $ python path/to/train.py --data coco128.yaml --weights yolov5s.pt --img 640\n",
    "\"\"\"\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pdb\n",
    "import argparse\n",
    "from label_converter import qt2yolo\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "from PIL.ImageFont import truetype\n",
    "from label_converter import yolo2bcc_new\n",
    "from train_with_bcc import convert_yolo2bcc, nn_predict, convert_cs_yolo2bcc\n",
    "# YOLOBCC\n",
    "from train_with_bcc import read_crowdsourced_labels, init_bcc_params, \\\n",
    "    init_nn_output, compute_param_confusion_matrices, init_metrics, update_bcc_metrics\n",
    "from lib.BCCNet.VariationalInference.VB_iteration_yolo import VB_iteration as VBi_yolo\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FILE = Path(__file__).absolute()\n",
    "# sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path\n",
    "\n",
    "import val  # for end-of-epoch mAP\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Model\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
    "    strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\n",
    "    check_requirements, print_mutation, set_logging, one_cycle, colorstr, methods\n",
    "from utils.downloads import attempt_download\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.plots import plot_labels, plot_evolve\n",
    "from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, intersect_dicts, select_device, \\\n",
    "    torch_distributed_zero_first\n",
    "from utils.loggers.wandb.wandb_utils import check_wandb_resume\n",
    "from utils.metrics import fitness\n",
    "from utils.loggers import Loggers\n",
    "from utils.callbacks import Callbacks\n",
    "\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n",
    "RANK = int(os.getenv('RANK', -1))\n",
    "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6caefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_opt(known=False):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--bcc', action='store_true', help='Whether to run YOLO with or without BCC.')\n",
    "    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')\n",
    "    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n",
    "    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='dataset.yaml path')\n",
    "    parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch.yaml', help='hyperparameters path')\n",
    "    parser.add_argument('--epochs', type=int, default=300)\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\n",
    "    parser.add_argument('--rect', action='store_true', help='rectangular training')\n",
    "    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')\n",
    "    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n",
    "    parser.add_argument('--noval', action='store_true', help='only validate final epoch')\n",
    "    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n",
    "    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')\n",
    "    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n",
    "    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in \"ram\" (default) or \"disk\"')\n",
    "    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\n",
    "    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')\n",
    "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\n",
    "    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\n",
    "    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')\n",
    "    parser.add_argument('--project', default='runs/train', help='save to project/name')\n",
    "    parser.add_argument('--entity', default=None, help='W&B entity')\n",
    "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--quad', action='store_true', help='quad dataloader')\n",
    "    parser.add_argument('--linear-lr', action='store_true', help='linear LR')\n",
    "    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')\n",
    "    parser.add_argument('--upload_dataset', action='store_true', help='Upload dataset as W&B artifact table')\n",
    "    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval for W&B')\n",
    "    parser.add_argument('--save_period', type=int, default=-1, help='Log model after every \"save_period\" epoch')\n",
    "    parser.add_argument('--artifact_alias', type=str, default=\"latest\", help='version of dataset artifact to be used')\n",
    "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\n",
    "    parser.add_argument('--freeze', type=int, default=0, help='Number of layers to freeze. backbone=10, all=24')\n",
    "    parser.add_argument('--patience', type=int, default=300, help='EarlyStopping patience (epochs)')\n",
    "    opt = parser.parse_known_args()[0] if known else parser.parse_args(\"\")\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab75535",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ec8153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ ed16fb7 torch 1.9.0 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mbcc=False, weights=yolov5s.pt, cfg=, data=data/toy.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=300, batch_size=10, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=0, project=runs/train, entity=None, name=exp, exist_ok=True, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=300\n"
     ]
    }
   ],
   "source": [
    "data_name = 'toy'\n",
    "opt = parse_opt()\n",
    "opt.data = f'data/{data_name}.yaml' # Does not make any difference!\n",
    "data_dict_path = f'../../datasets/{data_name}'\n",
    "opt.exist_ok = True\n",
    "opt.cache = None\n",
    "opt.workers = 0\n",
    "opt.batch_size = 10 # Change this to number of train images\n",
    "\n",
    "# Checks\n",
    "set_logging(RANK)\n",
    "if RANK in [-1, 0]:\n",
    "    print(colorstr('train: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\n",
    "#     check_git_status()\n",
    "#     check_requirements(requirements=FILE.parent / 'requirements.txt', exclude=['thop'])\n",
    "\n",
    "# Resume\n",
    "if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # resume an interrupted run\n",
    "    ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\n",
    "    assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\n",
    "    with open(Path(ckpt).parent.parent / 'opt.yaml') as f:\n",
    "        opt = argparse.Namespace(**yaml.safe_load(f))  # replace\n",
    "    opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate\n",
    "    LOGGER.info(f'Resuming training from {ckpt}')\n",
    "else:\n",
    "    opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check files\n",
    "    assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\n",
    "    if opt.evolve:\n",
    "        opt.project = 'runs/evolve'\n",
    "        opt.exist_ok = opt.resume\n",
    "    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))\n",
    "\n",
    "# DDP mode\n",
    "device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "if LOCAL_RANK != -1:\n",
    "    from datetime import timedelta\n",
    "    assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'\n",
    "    assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'\n",
    "    assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'\n",
    "    assert not opt.evolve, '--evolve argument is not compatible with DDP training'\n",
    "    torch.cuda.set_device(LOCAL_RANK)\n",
    "    device = torch.device('cuda', LOCAL_RANK)\n",
    "    dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\")\n",
    "hyp = opt.hyp\n",
    "callbacks=Callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a9801",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b14d77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-sharma\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">resilient-meadow-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/g-sharma/YOLOv5\" target=\"_blank\">https://wandb.ai/g-sharma/YOLOv5</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/g-sharma/YOLOv5/runs/dqxjn82y\" target=\"_blank\">https://wandb.ai/g-sharma/YOLOv5/runs/dqxjn82y</a><br/>\n",
       "                Run data is saved locally in <code>/Users/gs0029/repos/dental_disease/yolobcc/wandb/run-20210922_104247-dqxjn82y</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7066239 parameters, 7066239 gradients, 16.4 GFLOPs\n",
      "\n",
      "Transferred 356/362 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.00046875\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'datasets/toy/labels/train.cache' images and labels... 2 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'datasets/toy/labels/val.cache' images and labels... 2 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.25, Best Possible Recall (BPR) = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \\\n",
    "    Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n",
    "    opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n",
    "torchMode = True\n",
    "# Directories\n",
    "w = save_dir / 'weights'  # weights dir\n",
    "w.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "last, best = w / 'last.pt', w / 'best.pt'\n",
    "\n",
    "# Hyperparameters\n",
    "if isinstance(hyp, str):\n",
    "    with open(hyp) as f:\n",
    "        hyp = yaml.safe_load(f)  # load hyps dict\n",
    "LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\n",
    "\n",
    "# Save run settings\n",
    "with open(save_dir / 'hyp.yaml', 'w') as f:\n",
    "    yaml.safe_dump(hyp, f, sort_keys=False)\n",
    "with open(save_dir / 'opt.yaml', 'w') as f:\n",
    "    yaml.safe_dump(vars(opt), f, sort_keys=False)\n",
    "data_dict = {\n",
    "    'path': data_dict_path,  # dataset root dir\n",
    "    'train': 'images/train',  # train images (relative to 'path') 128 images\n",
    "    'val': 'images/val',  # val images (relative to 'path') 128 images\n",
    "    'test': 'images/test', # test images (optional)\n",
    "    'nc': 2,  # number of classes\n",
    "    'names': ['bone-loss', 'dental-caries']  # class names\n",
    "}\n",
    "for x in ['train', 'val', 'test']:\n",
    "    data_dict[x] = os.path.join(data_dict['path'], data_dict[x])\n",
    "\n",
    "# Loggers\n",
    "if RANK in [-1, 0]:\n",
    "    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n",
    "    if loggers.wandb:\n",
    "        data_dict = loggers.wandb.data_dict\n",
    "        if resume:\n",
    "            weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp\n",
    "\n",
    "    # Register actions\n",
    "    for k in methods(loggers):\n",
    "        callbacks.register_action(k, callback=getattr(loggers, k))\n",
    "\n",
    "# Config\n",
    "plots = not evolve  # create plots\n",
    "cuda = device.type != 'cpu'\n",
    "init_seeds(1 + RANK)\n",
    "with torch_distributed_zero_first(RANK):\n",
    "    data_dict = data_dict or check_dataset(data)  # check if None\n",
    "train_path, val_path = data_dict['train'], data_dict['val']\n",
    "nc = 1 if single_cls else int(data_dict['nc'])  # number of classes\n",
    "names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
    "assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check\n",
    "is_coco = data.endswith('coco.yaml') and nc == 80  # COCO dataset\n",
    "\n",
    "# Model\n",
    "pretrained = weights.endswith('.pt')\n",
    "if pretrained:\n",
    "    with torch_distributed_zero_first(RANK):\n",
    "        weights = attempt_download(weights)  # download if not found locally\n",
    "    ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "    model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "    exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys\n",
    "    csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "    csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n",
    "    model.load_state_dict(csd, strict=False)  # load\n",
    "    LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report\n",
    "else:\n",
    "    model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "\n",
    "# Freeze\n",
    "freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = True  # train all layers\n",
    "    if any(x in k for x in freeze):\n",
    "        print(f'freezing {k}')\n",
    "        v.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "nbs = 64  # nominal batch size\n",
    "accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
    "hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
    "LOGGER.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\n",
    "\n",
    "g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "for v in model.modules():\n",
    "    if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\n",
    "        g2.append(v.bias)\n",
    "    if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\n",
    "        g0.append(v.weight)\n",
    "    elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n",
    "        g1.append(v.weight)\n",
    "\n",
    "if opt.adam:\n",
    "    optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "else:\n",
    "    optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "\n",
    "optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "optimizer.add_param_group({'params': g2})  # add g2 (biases)\n",
    "LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups \"\n",
    "            f\"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias\")\n",
    "del g0, g1, g2\n",
    "\n",
    "# Scheduler\n",
    "if opt.linear_lr:\n",
    "    lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\n",
    "else:\n",
    "    lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
    "\n",
    "# EMA\n",
    "ema = ModelEMA(model) if RANK in [-1, 0] else None\n",
    "\n",
    "# Resume\n",
    "start_epoch, best_fitness = 0, 0.0\n",
    "if pretrained:\n",
    "    # Optimizer\n",
    "    if ckpt['optimizer'] is not None:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        best_fitness = ckpt['best_fitness']\n",
    "\n",
    "    # EMA\n",
    "    if ema and ckpt.get('ema'):\n",
    "        ema.ema.load_state_dict(ckpt['ema'].float().state_dict())\n",
    "        ema.updates = ckpt['updates']\n",
    "\n",
    "    # Epochs\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    if resume:\n",
    "        assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'\n",
    "    if epochs < start_epoch:\n",
    "        LOGGER.info(f\"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.\")\n",
    "        epochs += ckpt['epoch']  # finetune additional epochs\n",
    "\n",
    "    del ckpt, csd\n",
    "\n",
    "# Image sizes\n",
    "gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
    "imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
    "\n",
    "# DP mode\n",
    "if cuda and RANK == -1 and torch.cuda.device_count() > 1:\n",
    "    logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'\n",
    "                    'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# SyncBatchNorm\n",
    "if opt.sync_bn and cuda and RANK != -1:\n",
    "    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "    LOGGER.info('Using SyncBatchNorm()')\n",
    "\n",
    "#     print(\"*** GPU Usage before reading data\")\n",
    "#     gpu_usage()\n",
    "# Trainloader\n",
    "train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
    "                                          hyp=hyp, augment=False, cache=opt.cache, rect=opt.rect, rank=RANK,\n",
    "                                          workers=workers, image_weights=opt.image_weights, quad=opt.quad,\n",
    "                                          prefix=colorstr('train: '))\n",
    "# print(\"*** GPU Usage after reading data\")\n",
    "# gpu_usage()\n",
    "# YOLOBCC\n",
    "# TODO: Verify the following (is nl == ng?):\n",
    "n_grid_choices, n_anchor_choices = model.model[-1].nl, model.model[-1].na\n",
    "grid_ratios = model.model[-1].stride.cpu().detach().numpy() / imgsz\n",
    "cstargets_all = read_crowdsourced_labels(data)\n",
    "cstargets_all_bcc = convert_cs_yolo2bcc(cstargets_all, n_anchor_choices, nc, grid_ratios)\n",
    "# cstargets = cstargets_all['train']\n",
    "cstargets_bcc = torch.tensor(cstargets_all_bcc['train']) if torchMode else cstargets_all_bcc['train']\n",
    "# print(\"*** GPU Usage after reading crowdsourced labels\")\n",
    "# gpu_usage()\n",
    "mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class\n",
    "nb = len(train_loader)  # number of batches\n",
    "assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'\n",
    "\n",
    "# Process 0\n",
    "if RANK in [-1, 0]:\n",
    "    val_loader, val_dataset = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,\n",
    "                                   hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,\n",
    "                                   workers=workers, pad=0.5,\n",
    "                                   prefix=colorstr('val: '))\n",
    "\n",
    "    if not resume:\n",
    "        labels = np.concatenate(dataset.labels, 0)\n",
    "        # c = torch.tensor(labels[:, 0])  # classes\n",
    "        # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\n",
    "        # model._initialize_biases(cf.to(device))\n",
    "        if plots:\n",
    "            plot_labels(labels, names, save_dir)\n",
    "\n",
    "        # Anchors\n",
    "        if not opt.noautoanchor:\n",
    "            check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
    "        model.half().float()  # pre-reduce anchor precision\n",
    "\n",
    "    callbacks.on_pretrain_routine_end()\n",
    "\n",
    "# DDP mode\n",
    "if cuda and RANK != -1:\n",
    "    model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\n",
    "\n",
    "# Model parameters\n",
    "hyp['box'] *= 3. / nl  # scale to layers\n",
    "hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layers\n",
    "hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layers\n",
    "hyp['label_smoothing'] = opt.label_smoothing\n",
    "model.nc = nc  # attach number of classes to model\n",
    "model.hyp = hyp  # attach hyperparameters to model\n",
    "model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n",
    "model.names = names\n",
    "\n",
    "# Start training\n",
    "t0 = time.time()\n",
    "nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "# nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
    "last_opt_step = -1\n",
    "maps = np.zeros(nc)  # mAP per class\n",
    "results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "scaler = amp.GradScaler(enabled=cuda)\n",
    "stopper = EarlyStopping(patience=opt.patience)\n",
    "compute_loss = ComputeLoss(model)  # init loss class\n",
    "# YOLOBCC\n",
    "bcc_params = init_bcc_params()\n",
    "bcc_params['n_epoch'] = epochs\n",
    "batch_pcm = {k: torch.tensor(v).to(device) if torchMode else v for k, v in compute_param_confusion_matrices(bcc_params).items()}\n",
    "# pred0_bcc = init_nn_output(dataset.n, grid_ratios, n_anchor_choices, bcc_params)\n",
    "bcc_metrics = init_metrics(bcc_params['n_epoch'])\n",
    "LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\\n'\n",
    "            f'Using {train_loader.num_workers} dataloader workers\\n'\n",
    "            f\"Logging results to {colorstr('bold', save_dir)}\\n\"\n",
    "            f'Starting training for {epochs} epochs...')\n",
    "\n",
    "\n",
    "# print(\"GPU Usage after emptying the cache\")\n",
    "# torch.cuda.empty_cache()\n",
    "# gpu_usage()\n",
    "\n",
    "times = defaultdict(float)\n",
    "epoch_times = {epoch: defaultdict(float) for epoch in range(start_epoch, epochs)}\n",
    "batch_times = {i: defaultdict(float) for i in range(1 + np.max(dataset.batch))}\n",
    "epoch_batch_times = {epoch: {i: defaultdict(float) for i in batch_times} for epoch in epoch_times}\n",
    "for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "#     print(f\"*** GPU Usage before epoch {epoch} in {{{start_epoch}, ..., {epochs-1}}}\")\n",
    "#     gpu_usage()\n",
    "    LBs = []\n",
    "    model.train()\n",
    "    # if epoch == start_epoch:\n",
    "    #     pred_bcc = pred0_bcc\n",
    "\n",
    "    # Update image weights (optional, single-GPU only)\n",
    "    if opt.image_weights:\n",
    "        cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\n",
    "        iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\n",
    "        dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\n",
    "\n",
    "    # Update mosaic border (optional)\n",
    "    # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "    # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "    # GOVIND0: Find mean losses by comparing pred with `qtargets`, not `targets`. Look for pointer GOVIND1\n",
    "    mloss = torch.zeros(3, device=device)  # mean losses\n",
    "    if RANK != -1:\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "    pbar = enumerate(train_loader)\n",
    "    LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "    if RANK in [-1, 0]:\n",
    "        pbar = tqdm(pbar, total=nb)  # progress bar\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # YOLOBCC\n",
    "    # qtargets, pcm['variational'], lb = VBi_yolo(cstargets_bcc, pred_bcc, pcm['variational'], pcm['prior'])\n",
    "    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "        batch_cstargets_bcc = (cstargets_bcc[dataset.batch == i]).to(device)\n",
    "        # YOLOBCC\n",
    "        # batch_qtargets = qtargets\n",
    "        ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "        imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "        # Warmup\n",
    "        if ni <= nw:\n",
    "            xi = [0, nw]  # x interp\n",
    "            # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "            accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "            for j, x in enumerate(optimizer.param_groups):\n",
    "                # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                if 'momentum' in x:\n",
    "                    x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "        # Multi-scale\n",
    "        if opt.multi_scale:\n",
    "            sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "            sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "            if sf != 1:\n",
    "                ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Forward\n",
    "        with amp.autocast(enabled=cuda):\n",
    "            model.eval()\n",
    "\n",
    "            t1 = timer()\n",
    "            batch_pred_yolo = nn_predict(model, imgs, imgsz, transform_format_flag=False)\n",
    "            t2 = timer()\n",
    "            delta_t = round(t2 - t1, 2)\n",
    "            epoch_batch_times[epoch][i]['nn_predict'] += delta_t\n",
    "            epoch_times[epoch]['nn_predict'] += delta_t\n",
    "            batch_times[i]['nn_predict'] += delta_t\n",
    "            times['nn_predict'] += delta_t\n",
    "\n",
    "            t1 = timer()\n",
    "            batch_pred_bcc, batch_pred_yolo_wh = yolo2bcc_new(batch_pred_yolo, imgsz)\n",
    "            t2 = timer()\n",
    "            delta_t = round(t2 - t1, 2)\n",
    "            epoch_batch_times[epoch][i]['yolo2bcc_new'] += delta_t\n",
    "            epoch_times[epoch]['yolo2bcc_new'] += delta_t\n",
    "            batch_times[i]['yolo2bcc_new'] += delta_t\n",
    "            times['yolo2bcc_new'] += delta_t\n",
    "\n",
    "            t1 = timer()\n",
    "            batch_qtargets, batch_pcm['variational'], batch_lb = VBi_yolo(batch_cstargets_bcc, batch_pred_bcc, batch_pcm['variational'], batch_pcm['prior'], torchMode = torchMode, device=device)\n",
    "            t2 = timer()\n",
    "            delta_t = round(t2 - t1, 2)\n",
    "            epoch_batch_times[epoch][i]['VBi_yolo'] += delta_t\n",
    "            epoch_times[epoch]['VBi_yolo'] += delta_t\n",
    "            batch_times[i]['VBi_yolo'] += delta_t\n",
    "            times['VBi_yolo'] += delta_t\n",
    "\n",
    "            LBs.append(batch_lb)\n",
    "            \n",
    "            break\n",
    "            \n",
    "            t1 = timer()\n",
    "            batch_qtargets_yolo = qt2yolo(batch_qtargets, grid_ratios, n_anchor_choices, batch_pred_yolo_wh, torchMode = torchMode, device=device).half().float()\n",
    "            t2 = timer()\n",
    "            delta_t = round(t2 - t1, 2)\n",
    "            epoch_batch_times[epoch][i]['qt2yolo'] += delta_t\n",
    "            epoch_times[epoch]['qt2yolo'] += delta_t\n",
    "            batch_times[i]['qt2yolo'] += delta_t\n",
    "            times['qt2yolo'] += delta_t\n",
    "            # pred_bcc = convert_yolo2bcc(pred_yolo.cpu().detach().numpy(), n_anchor_choices, nc, grid_ratios, intermediate_yolo_mode=True)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            t1 = timer()\n",
    "            pred = model(imgs)  # forward\n",
    "            t2 = timer()\n",
    "            delta_t = round(t2 - t1, 2)\n",
    "            epoch_batch_times[epoch][i]['pred'] += delta_t\n",
    "            epoch_times[epoch]['pred'] += delta_t\n",
    "            batch_times[i]['pred'] += delta_t\n",
    "            times['pred'] += delta_t\n",
    "\n",
    "            # GOVIND1: Don't compare w.r.t. targets, but w.r.t. qtargets (generated by BCC)\n",
    "            # YOLOBCC\n",
    "            # loss, loss_items = compute_loss(pred, targets)  # loss scaled by batch_size\n",
    "            t1 = timer()\n",
    "            loss, loss_items = compute_loss(pred, batch_qtargets_yolo)\n",
    "            t2 = timer()\n",
    "            delta_t = round(t2 - t1, 2)\n",
    "            epoch_batch_times[epoch][i]['compute_loss'] += delta_t\n",
    "            epoch_times[epoch]['compute_loss'] += delta_t\n",
    "            batch_times[i]['compute_loss'] += delta_t\n",
    "            times['compute_loss'] += delta_t\n",
    "\n",
    "            if RANK != -1:\n",
    "                loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
    "            if opt.quad:\n",
    "                loss *= 4.\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cf1994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 25200, 2]),\n",
       " array([     0.0125,       0.025,        0.05], dtype=float32),\n",
       " 3,\n",
       " torch.Size([2, 25200, 2]),\n",
       " True,\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_qtargets.shape, grid_ratios, n_anchor_choices, batch_pred_yolo_wh.shape, torchMode, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278b0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_converter import qt2yolo_optimized\n",
    "qt_yolo1 = qt2yolo(batch_qtargets, grid_ratios, n_anchor_choices, batch_pred_yolo_wh, torchMode = torchMode, device=device).half().float()\n",
    "with torch.no_grad():\n",
    "    qt_yolo2 = qt2yolo_optimized(batch_qtargets, grid_ratios, n_anchor_choices, batch_pred_yolo_wh, torchMode = torchMode).half().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d378510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.00625, 0.00625, 0.02138, 0.00521],\n",
       "        [0.00000, 0.00000, 0.01875, 0.00625, 0.02017, 0.01564],\n",
       "        [0.00000, 0.00000, 0.03125, 0.00625, 0.01668, 0.01203],\n",
       "        ...,\n",
       "        [1.00000, 1.00000, 0.87500, 0.97510, 0.58447, 0.58301],\n",
       "        [1.00000, 1.00000, 0.92480, 0.97510, 0.53223, 0.53613],\n",
       "        [1.00000, 1.00000, 0.97510, 0.97510, 0.62109, 0.55029]], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt_yolo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64af8c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(qt_yolo1, qt_yolo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08978ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 25200, 2]),\n",
       " tensor([[[0.94075, 0.05925],\n",
       "          [0.92852, 0.07148],\n",
       "          [0.91696, 0.08304],\n",
       "          ...,\n",
       "          [0.42612, 0.57388],\n",
       "          [0.43887, 0.56113],\n",
       "          [0.44963, 0.55037]],\n",
       " \n",
       "         [[0.93847, 0.06153],\n",
       "          [0.92252, 0.07748],\n",
       "          [0.91223, 0.08777],\n",
       "          ...,\n",
       "          [0.42407, 0.57593],\n",
       "          [0.44052, 0.55948],\n",
       "          [0.44128, 0.55872]]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_qtargets.shape, batch_qtargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9a78b07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_thres = 0.75\n",
    "p = batch_qtargets.reshape(qt_yolo.shape[0], 2)\n",
    "(-p*p.log2()).sum(axis=1) <= entropy_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a351aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.hist((-p*p.log2()).sum(axis=1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b5a53817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-141-d49e884fc16f>:2: RuntimeWarning: divide by zero encountered in log2\n",
      "  p, (-p*np.log2(p)).sum(axis=1)\n",
      "<ipython-input-141-d49e884fc16f>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  p, (-p*np.log2(p)).sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[          0,           1],\n",
       "        [        0.1,         0.9],\n",
       "        [        0.2,         0.8],\n",
       "        [        0.3,         0.7],\n",
       "        [        0.4,         0.6],\n",
       "        [        0.5,         0.5],\n",
       "        [        0.6,         0.4],\n",
       "        [        0.7,         0.3],\n",
       "        [        0.8,         0.2],\n",
       "        [        0.9,         0.1]]),\n",
       " array([        nan,       0.469,     0.72193,     0.88129,     0.97095,           1,     0.97095,     0.88129,     0.72193,       0.469]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[x/10, 1-x/10] for x in range(10)])\n",
    "p, (-p*np.log2(p)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9905e85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 25200, 2]),\n",
       " tensor([[[0.94075, 0.05925],\n",
       "          [0.92852, 0.07148],\n",
       "          [0.91696, 0.08304],\n",
       "          ...,\n",
       "          [0.42612, 0.57388],\n",
       "          [0.43887, 0.56113],\n",
       "          [0.44963, 0.55037]],\n",
       " \n",
       "         [[0.93847, 0.06153],\n",
       "          [0.92252, 0.07748],\n",
       "          [0.91223, 0.08777],\n",
       "          ...,\n",
       "          [0.42407, 0.57593],\n",
       "          [0.44052, 0.55948],\n",
       "          [0.44128, 0.55872]]]),\n",
       " torch.Size([2, 25200]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt.shape, qt, conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d242070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "27bc9d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50400, 6]),\n",
       " tensor([[0.00000, 0.00000, 0.00625, 0.00625, 0.02138, 0.00521],\n",
       "         [0.00000, 0.00000, 0.01875, 0.00625, 0.02017, 0.01564],\n",
       "         [0.00000, 0.00000, 0.03125, 0.00625, 0.01668, 0.01203],\n",
       "         ...,\n",
       "         [1.00000, 1.00000, 0.87500, 0.97510, 0.58447, 0.58301],\n",
       "         [1.00000, 1.00000, 0.92480, 0.97510, 0.53223, 0.53613],\n",
       "         [1.00000, 1.00000, 0.97510, 0.97510, 0.62109, 0.55029]], grad_fn=<CopyBackwards>))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt_yolo.shape, qt_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d6984b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.05624, 0.06873, 0.00578, 0.05725],\n",
       "        [0.00000, 0.00000, 0.06873, 0.06873, 0.00478, 0.05463],\n",
       "        [0.00000, 0.00000, 0.08124, 0.06873, 0.00408, 0.04559],\n",
       "        ...,\n",
       "        [1.00000, 1.00000, 0.87500, 0.97510, 0.58447, 0.58301],\n",
       "        [1.00000, 1.00000, 0.92480, 0.97510, 0.53223, 0.53613],\n",
       "        [1.00000, 1.00000, 0.97510, 0.97510, 0.62109, 0.55029]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# high-conf-entropy\n",
    "conf_thres = 0.01\n",
    "qt_yolo[(conf >= conf_thres).reshape(qt_yolo.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c61027f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 1.00000, 0.47510, 0.12500, 0.46851, 0.19043],\n",
       "        [0.00000, 1.00000, 0.37500, 0.17505, 0.50000, 0.16528],\n",
       "        [0.00000, 1.00000, 0.32495, 0.17505, 0.60400, 0.22375],\n",
       "        [0.00000, 0.00000, 0.42505, 0.12500, 0.31421, 0.17004],\n",
       "        [0.00000, 0.00000, 0.42505, 0.17505, 0.26562, 0.14038],\n",
       "        [0.00000, 1.00000, 0.87500, 0.82520, 0.18457, 0.21130],\n",
       "        [0.00000, 1.00000, 0.47510, 0.17505, 0.33813, 0.19556],\n",
       "        [0.00000, 0.00000, 0.37500, 0.12500, 0.50928, 0.20435],\n",
       "        [0.00000, 1.00000, 0.42505, 0.32495, 0.51318, 0.10480],\n",
       "        [0.00000, 1.00000, 0.92480, 0.82520, 0.16931, 0.17725],\n",
       "        [1.00000, 1.00000, 0.87500, 0.77490, 0.71338, 0.39648],\n",
       "        [1.00000, 1.00000, 0.17505, 0.82520, 0.60938, 0.45947],\n",
       "        [1.00000, 1.00000, 0.22498, 0.82520, 0.78906, 0.44482],\n",
       "        [1.00000, 1.00000, 0.07501, 0.82520, 0.69336, 0.29785],\n",
       "        [1.00000, 1.00000, 0.32495, 0.82520, 0.87354, 0.32422],\n",
       "        [1.00000, 1.00000, 0.02499, 0.82520, 0.68457, 0.40405],\n",
       "        [1.00000, 1.00000, 0.37500, 0.82520, 0.97754, 0.31885],\n",
       "        [1.00000, 1.00000, 0.27490, 0.82520, 0.79688, 0.39502],\n",
       "        [1.00000, 1.00000, 0.92480, 0.22498, 0.32593, 0.10980],\n",
       "        [1.00000, 1.00000, 0.92480, 0.17505, 0.42041, 0.12561]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_thres = 10\n",
    "# top_indices = conf.reshape(qt_yolo.shape[0]).sort(descending=True).indices[:count_thres]\n",
    "# qt_yolo[top_indices, :]\n",
    "n_images = conf.shape[0]\n",
    "n_boxes = conf.shape[1]\n",
    "top_indices = conf.sort(descending=True).indices[:, :count_thres]\n",
    "indices = top_indices + torch.tensor([[i*n_boxes for i in range(n_images)]]).transpose(1, 0)\n",
    "indices = indices.reshape(indices.shape[0]*indices.shape[1])\n",
    "return qt_yolo[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "996bc9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 25200]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_images = conf.shape[0]\n",
    "n_boxes = conf.shape[1]\n",
    "[i*n_boxes for i in range(n_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f845b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50400, 6]), torch.Size([2932, 6]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt_yolo = batch_qtargets_yolo\n",
    "conf = batch_pred_yolo[:, :, 4]\n",
    "\n",
    "conf_thres = 0.01\n",
    "n_images = conf.shape[0]\n",
    "# for i in range(n_images):\n",
    "i = 0\n",
    "from collections import Counter\n",
    "qt_yolo.shape, qt_yolo[(conf > conf_thres).reshape(qt_yolo.shape[0])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0240fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qtargets_yolo = qt2yolo_optimized(batch_qtargets, grid_ratios, n_anchor_choices, batch_pred_yolo_wh, torchMode = torchMode).half().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0040debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GRID_CELLS = 99999\n",
    "BACKGROUND_CLASS_ID = -1\n",
    "\n",
    "def find_grid_center(grid_x_frac, grid_y_frac, gc, mode='row-major'):\n",
    "    if grid_x_frac == 0 or grid_y_frac == 0:\n",
    "        raise Exception(\n",
    "            \"Cannot divide into infinite grids. Ensure grid fractions are > 0.\")\n",
    "    n_x_cells, n_y_cells = np.ceil(1 / grid_x_frac).astype(int), np.ceil(1 / grid_y_frac).astype(int)\n",
    "    if n_x_cells*n_y_cells > MAX_GRID_CELLS:\n",
    "        raise Exception(\"Grid cells maxed out.\")\n",
    "    if mode.startswith('row'):\n",
    "        x_cell_id, y_cell_id = gc % n_x_cells, np.floor(gc / n_x_cells).astype(int)\n",
    "    elif mode.startswith('col'):\n",
    "        x_cell_id, y_cell_id = np.floor(gc / n_y_cells).astype(int), gc % n_y_cells\n",
    "    else:\n",
    "        raise Exception(\"Incorrect mode for finding grid center.\")\n",
    "    x = x_cell_id*grid_x_frac + grid_x_frac/2\n",
    "    y = y_cell_id*grid_y_frac + grid_y_frac/2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56a27a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49ed45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.12500, 0.12500, 0.02138, 0.00521],\n",
       "        [0.00000, 0.00000, 0.37500, 0.12500, 0.02017, 0.01564],\n",
       "        [0.00000, 0.00000, 0.62500, 0.12500, 0.01668, 0.01203],\n",
       "        [0.00000, 0.00000, 0.87500, 0.12500, 0.01415, 0.01787],\n",
       "        [0.00000, 0.00000, 0.12500, 0.37500, 0.01383, 0.02351],\n",
       "        [0.00000, 0.00000, 0.37500, 0.37500, 0.01384, 0.02454],\n",
       "        [0.00000, 0.00000, 0.62500, 0.37500, 0.01687, 0.02681],\n",
       "        [0.00000, 0.00000, 0.87500, 0.37500, 0.01815, 0.02719],\n",
       "        [0.00000, 0.00000, 0.12500, 0.62500, 0.01826, 0.02552],\n",
       "        [0.00000, 0.00000, 0.37500, 0.62500, 0.01929, 0.02550],\n",
       "        [0.00000, 0.00000, 0.62500, 0.62500, 0.01783, 0.02484],\n",
       "        [0.00000, 0.00000, 0.87500, 0.62500, 0.01832, 0.02437],\n",
       "        [0.00000, 0.00000, 0.12500, 0.87500, 0.01727, 0.02172],\n",
       "        [0.00000, 0.00000, 0.37500, 0.87500, 0.01810, 0.02072],\n",
       "        [0.00000, 0.00000, 0.62500, 0.87500, 0.01606, 0.01922],\n",
       "        [0.00000, 0.00000, 0.87500, 0.87500, 0.01708, 0.01914]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt = batch_qtargets\n",
    "G = grid_ratios\n",
    "Na = n_anchor_choices\n",
    "wh_yolo = batch_pred_yolo_wh\n",
    "\n",
    "Ng = G.shape[0]\n",
    "num_images = qt.shape[0]\n",
    "y_bcc = []\n",
    "# for i in range(num_images):\n",
    "i = 0\n",
    "cs = torch.argmax(qt[i, :], 1)\n",
    "st = 0\n",
    "# for g in range(Ng):\n",
    "g = 0\n",
    "g_frac = 0.25#G[g]\n",
    "S_g = np.ceil(1/g_frac).astype(int)\n",
    "n_cells = S_g*S_g\n",
    "# for a in range(Na):\n",
    "a = 0\n",
    "z = torch.linspace(g_frac/2, 1-g_frac/2, S_g).repeat(S_g, 1).unsqueeze(-1)\n",
    "xy = torch.cat((z.permute(1, 0, 2), z), 2).permute(1, 0, 2).reshape(n_cells, 2)\n",
    "wh = wh_yolo[i][st:st+n_cells]\n",
    "c = cs[st:st+n_cells]\n",
    "icxywh = torch.cat((i*torch.ones(n_cells, 1), c.unsqueeze(-1), xy, wh), 1)\n",
    "y_bcc.append(icxywh)\n",
    "st += n_cells\n",
    "# torch.cat(y_bcc).shape\n",
    "icxywh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc8982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.00625, 0.00625, 0.02138, 0.00521],\n",
       "        [0.00000, 0.00000, 0.00625, 0.01875, 0.02017, 0.01564],\n",
       "        [0.00000, 0.00000, 0.00625, 0.03125, 0.01668, 0.01203],\n",
       "        ...,\n",
       "        [0.00000, 0.00000, 0.99375, 0.96875, 0.02559, 0.00252],\n",
       "        [0.00000, 0.00000, 0.99375, 0.98125, 0.02765, 0.00269],\n",
       "        [0.00000, 0.00000, 0.99375, 0.99375, 0.01773, 0.00155]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cfd031d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 20853, 1: 4347}), Counter({0: 21489, 1: 3711}))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(torch.argmax(qt, 2)[0].cpu().detach().numpy()), Counter(torch.argmax(qt, 2)[1].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20253af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 20853, 1: 4347}), Counter({0: 21489, 1: 3711}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(torch.argmax(qt[0, :], 1).cpu().detach().numpy()), Counter(torch.argmax(qt[1, :], 1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9cf24baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.00625, 0.00625, 0.02138, 0.00521],\n",
       "        [0.00000, 0.00000, 0.01875, 0.00625, 0.02017, 0.01564],\n",
       "        [0.00000, 0.00000, 0.03125, 0.00625, 0.01668, 0.01203],\n",
       "        ...,\n",
       "        [1.00000, 1.00000, 0.87500, 0.97500, 0.58438, 0.58290],\n",
       "        [1.00000, 1.00000, 0.92500, 0.97500, 0.53225, 0.53605],\n",
       "        [1.00000, 1.00000, 0.97500, 0.97500, 0.62085, 0.55034]], dtype=torch.float64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt = batch_qtargets\n",
    "G = grid_ratios\n",
    "Na = n_anchor_choices\n",
    "wh_yolo = batch_pred_yolo_wh\n",
    "\n",
    "Ng = G.shape[0]\n",
    "y_bcc = []\n",
    "num_images = qt.shape[0]\n",
    "for i in range(num_images):\n",
    "    effective_id = 0\n",
    "    cs = torch.argmax(qt[i, :], 1) if torchMode else np.argmax(qt[i, :], 1)\n",
    "    for g in range(Ng):\n",
    "        g_frac = G[g]\n",
    "        S_g = np.ceil(1/g_frac).astype(int)\n",
    "        for a in range(Na):\n",
    "            for gc in range(S_g*S_g):\n",
    "                x, y = find_grid_center(g_frac, g_frac, gc)\n",
    "                w, h = wh_yolo[i][effective_id]\n",
    "                c = cs[effective_id]\n",
    "                y_bcc.append([i, c, x, y, w, h])\n",
    "                effective_id += 1\n",
    "torch.tensor(y_bcc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "145c80fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50400, 6]),\n",
       " tensor([[0.00000, 0.00000, 0.00625, 0.00625, 0.02138, 0.00521],\n",
       "         [0.00000, 0.00000, 0.01875, 0.00625, 0.02017, 0.01564],\n",
       "         [0.00000, 0.00000, 0.03125, 0.00625, 0.01668, 0.01203],\n",
       "         ...,\n",
       "         [1.00000, 1.00000, 0.87500, 0.97500, 0.58438, 0.58290],\n",
       "         [1.00000, 1.00000, 0.92500, 0.97500, 0.53225, 0.53605],\n",
       "         [1.00000, 1.00000, 0.97500, 0.97500, 0.62085, 0.55034]], dtype=torch.float64))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(y_bcc), torch.cat(y_bcc).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtargets, pcm_var, lb = VBi_yolo(cstargets_bcc, pred_bcc, pcm['variational'], pcm['prior'])\n",
    "qtargets, pcm_var, lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "94b56b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# part of computing loss\n",
    "def logB_from_Dirichlet_parameters(alpha):\n",
    "    torchMode = torch.is_tensor(alpha)\n",
    "    if torchMode:\n",
    "        base_lib = torch\n",
    "        gammaln_fn = torch.special.gammaln\n",
    "        simple_transpose = lambda x: torch.transpose(x, 0, 1)\n",
    "    else:\n",
    "        base_lib = np\n",
    "        gammaln_fn = ss.gammaln\n",
    "        simple_transpose = np.transpose\n",
    "    logB = base_lib.sum(gammaln_fn(alpha)) - gammaln_fn(base_lib.sum(alpha))\n",
    "\n",
    "    return logB\n",
    "\n",
    "\n",
    "def expected_log_Dirichlet_parameters(param):\n",
    "    torchMode = torch.is_tensor(param)\n",
    "    if torchMode:\n",
    "        base_lib = torch\n",
    "        digamma_fn = torch.digamma\n",
    "        simple_transpose = lambda x: torch.transpose(x, 0, 1)\n",
    "    else:\n",
    "        base_lib = np\n",
    "        digamma_fn = ss.psi\n",
    "        simple_transpose = np.transpose\n",
    "        \n",
    "    size = param.shape\n",
    "    result = base_lib.zeros_like(param)\n",
    "\n",
    "    if len(size) == 1:\n",
    "        result = digamma_fn(param) - digamma_fn(base_lib.sum(param))\n",
    "    elif len(size) == 2:  # when we take A_0 for everyone\n",
    "        result = digamma_fn(param) - simple_transpose(base_lib.tile(digamma_fn(base_lib.sum(param, 1)), (size[1], 1)))\n",
    "    elif len(size) == 3:  # most of time for posterior cm\n",
    "        for i in range(size[2]):\n",
    "            result[:, :, i] = digamma_fn(param[:, :, i]) - \\\n",
    "                              simple_transpose(base_lib.tile(digamma_fn(base_lib.sum(param[:, :, i], 1)), (size[1], 1)))\n",
    "    else:\n",
    "        raise Exception('param can have no more than 3 dimensions')\n",
    "    return result\n",
    "\n",
    "\n",
    "def expected_true_labels(X, nn_output, ElogPi_volunteer):\n",
    "    def max_fun(t1, t2):\n",
    "        if not torch.is_tensor(t1):\n",
    "            t1 = torch.tensor(t1)\n",
    "        if not torch.is_tensor(t2):\n",
    "            t2 = torch.tensor(t2)\n",
    "        return torch.max(t1, t2)\n",
    "    torchMode = all([torch.is_tensor(x) for x in [X, nn_output, ElogPi_volunteer]])\n",
    "    if torchMode:\n",
    "        torch_invert_dims = lambda x: torch.permute(x, tuple(range(x.ndim)[::-1]))\n",
    "        base_lib = torch\n",
    "        digamma_fn = torch.digamma\n",
    "        copy_fn = lambda x: x.clone().detach()\n",
    "        simple_transpose = torch_invert_dims\n",
    "        maxwithdim_fn = lambda x, d: torch.max(x, d).values\n",
    "        maximum_fn = lambda t1, t2: max_fun(t1, t2)\n",
    "    else:\n",
    "        base_lib = np\n",
    "        digamma_fn = ss.psi\n",
    "        copy_fn = np.copy\n",
    "        simple_transpose = np.transpose\n",
    "        maxwithdim_fn = np.max\n",
    "        maximum_fn = np.maximum\n",
    "\n",
    "    I, U, K = X.shape  # I = no. of image, U = no. of anchor boxes in total, K = no. of volunteers\n",
    "    M = ElogPi_volunteer.shape[0]  # M = Number of classes\n",
    "    N = ElogPi_volunteer.shape[1]  # N = Number of classes used by volunteers\n",
    "\n",
    "    rho = copy_fn(nn_output)  # I x U x M logits\n",
    "    # eq. 12:\n",
    "    for k in range(K):\n",
    "        inds = base_lib.where(X[:, :, k] > -1)  # rule out missing values\n",
    "        rho[inds[0], inds[1], :] += simple_transpose(\n",
    "            ElogPi_volunteer[:, base_lib.squeeze(X[inds[0], inds[1], k]), k])\n",
    "\n",
    "    # normalisation: (minus the max of each anchor)\n",
    "    rho -= simple_transpose(base_lib.tile(simple_transpose(maxwithdim_fn(rho, 2)), (M, 1, 1)))\n",
    "\n",
    "    # # eq. 11:\n",
    "    q_t = base_lib.exp(rho) / maximum_fn(1e-60, simple_transpose(base_lib.tile(simple_transpose(base_lib.sum(base_lib.exp(rho), 2)), (M, 1, 1))))\n",
    "    q_t = maximum_fn(1e-60, q_t)\n",
    "\n",
    "    # partial of eq. 8: (right side 2nd term)\n",
    "    f_iu = base_lib.zeros((M, N, K), dtype=base_lib.float64)\n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            ids0 = base_lib.where(X[:, :, k] == n)[0]\n",
    "            ids1 = base_lib.where(X[:, :, k] == n)[1]\n",
    "            f_iu[:, n, k] = base_lib.sum(q_t[ids0, ids1, :], 0)\n",
    "    rho.shape, rho\n",
    "    return q_t, f_iu, rho\n",
    "# dim: (I x U x M), (M x N x K), (I x U x M)\n",
    "\n",
    "\n",
    "# eq. 8:\n",
    "def update_alpha_volunteers(alpha0_volunteers, f_iu):\n",
    "    torchMode = all([torch.is_tensor(x) for x in [alpha0_volunteers, f_iu]])\n",
    "    if torchMode:\n",
    "        base_lib = torch\n",
    "    else:\n",
    "        base_lib = np\n",
    "    K = alpha0_volunteers.shape[2]\n",
    "    alpha_volunteers = base_lib.zeros_like(alpha0_volunteers)\n",
    "\n",
    "    for k in range(K):\n",
    "        alpha_volunteers[:, :, k] = alpha0_volunteers[:, :, k] + f_iu[:, :, k]\n",
    "\n",
    "    return alpha_volunteers\n",
    "\n",
    "\n",
    "def compute_lower_bound_likelihood(alpha0_volunteers, alpha_volunteers, q_t, rho, nn_output):\n",
    "    torchMode = all([torch.is_tensor(x) for x in [alpha0_volunteers, alpha_volunteers, q_t, rho, nn_output]])\n",
    "    if torchMode:\n",
    "        base_lib = torch\n",
    "    else:\n",
    "        base_lib = np\n",
    "    \n",
    "    W = alpha0_volunteers.shape[2]\n",
    "\n",
    "    ll_pi_worker = 0\n",
    "    for w in range(W):\n",
    "        ll_pi_worker -= base_lib.sum(logB_from_Dirichlet_parameters(alpha0_volunteers[:, :, w]) -\n",
    "                                             logB_from_Dirichlet_parameters(alpha_volunteers[:, :, w]))\n",
    "\n",
    "    ll_t = -base_lib.sum(q_t * rho) + base_lib.sum(base_lib.log(base_lib.sum(base_lib.exp(rho), axis=1)), axis=0)\n",
    "\n",
    "    ll_nn = base_lib.sum(q_t * nn_output) - base_lib.sum(base_lib.log(base_lib.sum(base_lib.exp(nn_output), axis=1)), axis=0)\n",
    "\n",
    "    ll = ll_pi_worker + ll_t + ll_nn  # VB lower bound\n",
    "\n",
    "    return base_lib.sum(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f7b10619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2pt(np, device):\n",
    "    return torch.tensor(np).to(device)\n",
    "\n",
    "def pt2np(pt):\n",
    "    return pt.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb0760",
   "metadata": {},
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "75aa1cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[    0.93847,    0.061533],\n",
       "         [    0.92252,    0.077477],\n",
       "         [    0.91223,    0.087769],\n",
       "         ...,\n",
       "         [    0.42407,     0.57593],\n",
       "         [    0.44052,     0.55948],\n",
       "         [    0.44128,     0.55872]],\n",
       " \n",
       "        [[    0.94075,    0.059253],\n",
       "         [    0.92852,     0.07148],\n",
       "         [    0.91696,    0.083039],\n",
       "         ...,\n",
       "         [    0.42612,     0.57388],\n",
       "         [    0.43887,     0.56113],\n",
       "         [    0.44963,     0.55037]]], dtype=float32),\n",
       " array([[[     29.085,       11.94,      35.662,      30.173],\n",
       "         [          0,      14.694,           0,           0]],\n",
       " \n",
       "        [[     15.915,      6.0599,      18.338,      14.827],\n",
       "         [          0,      12.306,           0,           0]]]),\n",
       " array([[[          0,     -2.7247],\n",
       "         [          0,     -2.4771],\n",
       "         [          0,     -2.3412],\n",
       "         ...,\n",
       "         [   -0.30607,           0],\n",
       "         [   -0.23904,           0],\n",
       "         [   -0.23595,           0]],\n",
       " \n",
       "        [[          0,     -2.7649],\n",
       "         [          0,     -2.5642],\n",
       "         [          0,     -2.4018],\n",
       "         ...,\n",
       "         [   -0.29771,           0],\n",
       "         [   -0.24576,           0],\n",
       "         [   -0.20217,           0]]], dtype=float32))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.special as ss\n",
    "import numpy as np\n",
    "# import pdb\n",
    "\n",
    "X, nn_output, alpha_volunteers, alpha0_volunteers = cstargets_bcc, pred_bcc.cpu().detach().numpy(), \\\n",
    "    pcm['variational'], pcm['prior']\n",
    "\n",
    "# def VB_iteration(X, nn_output, alpha_volunteers, alpha0_volunteers):\n",
    "# pdb.set_trace()\n",
    "ElogPi_volunteer = expected_log_Dirichlet_parameters(alpha_volunteers)\n",
    "\n",
    "# q_t\n",
    "q_t, Njl, rho = expected_true_labels(X, nn_output, ElogPi_volunteer)\n",
    "\n",
    "# q_pi_workers\n",
    "# alpha_volunteers = update_alpha_volunteers(alpha0_volunteers, Njl)\n",
    "\n",
    "# # Low bound\n",
    "# lower_bound_likelihood = compute_lower_bound_likelihood(alpha0_volunteers, alpha_volunteers, \\\n",
    "#                                                         q_t, rho, nn_output)\n",
    "\n",
    "# qtargets, pcm_var, lb = q_t, alpha_volunteers, lower_bound_likelihood\n",
    "\n",
    "q_t, Njl, rho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f04756",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fea5013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.93847, 0.06153],\n",
       "          [0.92252, 0.07748],\n",
       "          [0.91223, 0.08777],\n",
       "          ...,\n",
       "          [0.42407, 0.57593],\n",
       "          [0.44052, 0.55948],\n",
       "          [0.44128, 0.55872]],\n",
       " \n",
       "         [[0.94075, 0.05925],\n",
       "          [0.92852, 0.07148],\n",
       "          [0.91696, 0.08304],\n",
       "          ...,\n",
       "          [0.42612, 0.57388],\n",
       "          [0.43887, 0.56113],\n",
       "          [0.44963, 0.55037]]]),\n",
       " tensor([[[29.08494, 11.94005, 35.66164, 30.17300],\n",
       "          [ 0.00000, 14.69351,  0.00000,  0.00000]],\n",
       " \n",
       "         [[15.91506,  6.05995, 18.33836, 14.82700],\n",
       "          [ 0.00000, 12.30649,  0.00000,  0.00000]]], dtype=torch.float64),\n",
       " tensor([[[ 0.00000, -2.72467],\n",
       "          [ 0.00000, -2.47713],\n",
       "          [ 0.00000, -2.34119],\n",
       "          ...,\n",
       "          [-0.30607,  0.00000],\n",
       "          [-0.23904,  0.00000],\n",
       "          [-0.23595,  0.00000]],\n",
       " \n",
       "         [[ 0.00000, -2.76486],\n",
       "          [ 0.00000, -2.56418],\n",
       "          [ 0.00000, -2.40175],\n",
       "          ...,\n",
       "          [-0.29771,  0.00000],\n",
       "          [-0.24576,  0.00000],\n",
       "          [-0.20217,  0.00000]]]))"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, nn_output, alpha_volunteers, alpha0_volunteers = torch.tensor(cstargets_bcc), pred_bcc, \\\n",
    "    torch.tensor(pcm['variational']), torch.tensor(pcm['prior'])\n",
    "\n",
    "# def VB_iteration_pytorch(X, nn_output, alpha_volunteers, alpha0_volunteers):\n",
    "\n",
    "# pdb.set_trace()\n",
    "ElogPi_volunteer = expected_log_Dirichlet_parameters(alpha_volunteers)\n",
    "\n",
    "# q_t\n",
    "q_t, Njl, rho = expected_true_labels(X, nn_output, ElogPi_volunteer)\n",
    "\n",
    "# q_pi_workers\n",
    "# alpha_volunteers = update_alpha_volunteers(alpha0_volunteers, Njl)\n",
    "\n",
    "# # Low bound\n",
    "# lower_bound_likelihood = compute_lower_bound_likelihood(alpha0_volunteers, alpha_volunteers, \\\n",
    "#                                                         q_t, rho, nn_output)\n",
    "\n",
    "# qtargets, pcm_var, lb = q_t, alpha_volunteers, lower_bound_likelihood\n",
    "\n",
    "q_t, Njl, rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd80e75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 25200, 7]),\n",
       " tensor([[[3.62112e-03, 9.27925e-03, 1.36846e+01,  ..., 1.49550e-03, 8.22292e-01, 5.17919e-02],\n",
       "          [1.59202e-02, 7.25159e-03, 1.29100e+01,  ..., 1.89948e-03, 8.26544e-01, 6.36292e-02],\n",
       "          [2.91756e-02, 7.79393e-03, 1.06776e+01,  ..., 1.34058e-03, 7.16163e-01, 6.48550e-02],\n",
       "          ...,\n",
       "          [8.87906e-01, 9.82400e-01, 3.69486e+02,  ..., 1.89169e-02, 3.72366e-01, 5.01492e-01],\n",
       "          [9.36504e-01, 9.79430e-01, 3.28244e+02,  ..., 1.73013e-02, 3.57602e-01, 4.57227e-01],\n",
       "          [9.83267e-01, 9.79760e-01, 3.76296e+02,  ..., 1.72529e-02, 3.72640e-01, 4.56131e-01]],\n",
       " \n",
       "         [[3.80318e-03, 8.85521e-03, 1.23594e+01,  ..., 1.40285e-03, 8.24831e-01, 5.40825e-02],\n",
       "          [1.61285e-02, 6.41171e-03, 1.08387e+01,  ..., 1.87619e-03, 8.26284e-01, 6.93943e-02],\n",
       "          [2.98106e-02, 6.88588e-03, 8.18122e+00,  ..., 1.46679e-03, 7.17260e-01, 6.90101e-02],\n",
       "          ...,\n",
       "          [8.84591e-01, 9.75536e-01, 3.74005e+02,  ..., 1.49167e-02, 4.05465e-01, 5.50655e-01],\n",
       "          [9.33291e-01, 9.73586e-01, 3.40639e+02,  ..., 1.48356e-02, 4.04855e-01, 5.14176e-01],\n",
       "          [9.83699e-01, 9.76865e-01, 3.97344e+02,  ..., 1.61515e-02, 4.06849e-01, 5.15116e-01]]], grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred_yolo = nn_predict(model, imgs, imgsz, transform_format_flag=False)\n",
    "pred_yolo.shape, pred_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1da436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_converter import yolo2bcc_new\n",
    "pred_bcc, pred_yolo_wh = yolo2bcc_new(pred_yolo)\n",
    "model.train()\n",
    "pred_yolo_wh = pred_yolo_wh.cpu().detach().numpy()\n",
    "pred = model(imgs)  # forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5a05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtargets, pcm_v, lb = VBi_yolo(cstargets_bcc, pred_bcc.detach().numpy(), pcm['variational'], pcm['prior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a9a8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 25200, 2), (2, 25200, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtargets.shape, pred_yolo_wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a9ce266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7167199148029819 0.2063380281690141 0.10862619808306709 0.18450704225352113\r\n",
      "1 0.6629392971246006 0.6330985915492958 0.10543130990415335 0.1732394366197183\r\n",
      "0 0.1847710330138445 0.6098591549295774 0.13099041533546327 0.15774647887323945\r\n",
      "1 0.5213219616204691 0.30277777777777776 0.31544562899786777 0.41095555555555563\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../../datasets/toy/labels/train/train*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7a638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d67c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6]),\n",
       " tensor([[0.00000, 1.00000, 0.52132, 0.34869, 0.31545, 0.31528],\n",
       "         [1.00000, 1.00000, 0.71672, 0.27838, 0.10863, 0.13925],\n",
       "         [1.00000, 1.00000, 0.66294, 0.60045, 0.10543, 0.13074],\n",
       "         [1.00000, 0.00000, 0.18477, 0.58291, 0.13099, 0.11905]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab88fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ecd643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          0,           0,     0.00625,     0.00625,      13.685,      3.3365],\n",
       "       [          0,           0,     0.01875,     0.00625,       12.91,      10.012],\n",
       "       [          0,           0,     0.03125,     0.00625,      10.678,      7.7009],\n",
       "       ...,\n",
       "       [          1,           1,       0.875,       0.975,      374.01,      373.05],\n",
       "       [          1,           1,       0.925,       0.975,      340.64,      343.07],\n",
       "       [          1,           1,       0.975,       0.975,      397.34,      352.22]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from label_converter import find_grid_center\n",
    "def qt2yolo(qt):\n",
    "    G = grid_ratios\n",
    "    Ng = G.shape[0]\n",
    "    Na = 3\n",
    "    y_bcc = []\n",
    "    num_images, num_grid_cells, _ = qt.shape\n",
    "    for i in range(num_images):\n",
    "        effective_id = 0\n",
    "        cs = np.argmax(qt[i, :], 1)            \n",
    "        for g in range(Ng):\n",
    "            g_frac = G[g]\n",
    "            S_g = np.ceil(1/g_frac).astype(int)\n",
    "            for a in range(Na):\n",
    "                for gc in range(S_g*S_g):\n",
    "                    x, y = find_grid_center(g_frac, g_frac, gc)\n",
    "                    w, h = pred_yolo_wh[i][effective_id]\n",
    "                    c = cs[effective_id]\n",
    "    #                 if (effective_id%100==0):\n",
    "    #                     print('I =', i, ', g =', g, ', a =', a, ', gc =', gc, qt[i, effective_id, :], \\\n",
    "    #                           (round(x, 2), round(y, 2), round(w, 2), round(h, 2), c))\n",
    "                    y_bcc.append([i, c, x, y, w, h])\n",
    "                    effective_id += 1\n",
    "    return np.array(y_bcc)\n",
    "\n",
    "qt2yolo(qtargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a454c4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[    0.97874,      2.2409],\n",
       "        [     1.8676,    -0.97728],\n",
       "        [    0.95009,    -0.15136],\n",
       "        ...,\n",
       "        [    0.79987,       1.579],\n",
       "        [   -0.47848,     -1.3616],\n",
       "        [    -1.1311,    -0.87371]],\n",
       "\n",
       "       [[    0.45091,    -0.99932],\n",
       "        [    -1.9487,     -1.0684],\n",
       "        [   -0.68979,     0.19958],\n",
       "        ...,\n",
       "        [    0.14034,    0.040687],\n",
       "        [    0.56578,     -2.6195],\n",
       "        [   -0.79674,    -0.92146]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred0_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e742b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "pil_img = Image(filename=os.path.join(data_dict['train'], 'train1.jpg'))\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ffd85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85817c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = targets[:,2:]\n",
    "# z[:, :2] = z[:, :2] - z[:, 2:]/2\n",
    "# z[:, 2:] = z[:, :2] + z[:, 2:]\n",
    "z*x_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images, _, x_size, y_size = imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc = torch.log(pred_yolo[:, ..., 5:]/pred_yolo[:,...,5:].sum(2).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yolo[:,...,5:].sum(2).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d50923",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qtargets.shape, batch_qtarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec089fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_bcc2yolo():\n",
    "bcc_labels = {'labels': batch_qtargets, 'Na': 3, 'Nc': 2, 'wh_map': }\n",
    "G, Na, Nc, wh_map = [bcc_labels[x] for x in ['G', 'Na', 'Nc', 'wh_map']]\n",
    "Ng = G.shape[0]\n",
    "S = (1/G).astype(int)\n",
    "C = list(range(Nc))\n",
    "effective_id = 0\n",
    "yolo_labels = []\n",
    "for g in range(Ng):\n",
    "    g_labels = []\n",
    "    g_frac = G[g]\n",
    "    cells_per_side = np.ceil(1/g_frac).astype(int)\n",
    "    for a in range(Na):\n",
    "        a_labels = []\n",
    "        for gc in range(cells_per_side**2):  # grid-cells\n",
    "            c = bcc_labels['labels'][effective_id]\n",
    "            effective_id += 1\n",
    "            if c == BACKGROUND_CLASS_ID:\n",
    "                continue\n",
    "            x, y = find_grid_center(g_frac, g_frac, gc)\n",
    "            # print(effective_id, c)\n",
    "            w, h = bcc_labels['wh_map'][(g, a, gc)]\n",
    "            gc_label = [c, x, y, w, h]\n",
    "            a_labels.append(gc_label)\n",
    "        g_labels.append(a_labels)\n",
    "    yolo_labels.append(g_labels)\n",
    "if return_flattened:\n",
    "    y = np.concatenate([np.concatenate(x) for x in yolo_labels])\n",
    "else:\n",
    "    y = np.array(yolo_labels)\n",
    "return {'labels': y, 'G': G, 'Nc': Nc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5279b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "imgplot = plt.imshow(np.transpose(imgs[1], (1, 2, 0)), cmap='gray')\n",
    "\n",
    "for i in range(3, z.shape[0]):\n",
    "    x, y, w, h = z[i]*x_size\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "imgplot = plt.imshow(np.transpose(imgs[0], (1, 2, 0)), cmap='gray')\n",
    "\n",
    "for i in range(3):\n",
    "    x, y, w, h = z[i]*x_size\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_t, pcm_var, lb = VBi_yolo(cstargets_bcc, pred_bcc.cpu().detach().numpy(), pcm['variational'], pcm['prior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z[:,...,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(cstargets_bcc[1][:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc = convert_yolo2bcc(pred_yolo.cpu().detach().numpy(), n_anchor_choices, nc, grid_ratios, intermediate_yolo_mode = True)\n",
    "pred_bcc.shape, pred_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e42192",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yolo.shape, pred_bcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(pred_yolo[...,0].cpu().detach().numpy()[1]), Counter(pred_bcc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(pred_yolo[0][:, 0].cpu().detach().numpy()), Counter(pred_bcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yolo[0, :4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pred_yolo.cpu().detach().numpy()\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2185bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d37dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "effective_id = 0\n",
    "points = {}\n",
    "for g, gr in enumerate(grid_ratios):\n",
    "    points[g] = {}\n",
    "    cells_per_side = np.ceil(1/gr).astype(int)\n",
    "    for a in range(3):\n",
    "        points[g][a] = []\n",
    "        for gc in range(cells_per_side**2):\n",
    "            points[g][a].append(y[i, effective_id, [1,2]])\n",
    "            effective_id += 1\n",
    "        points[g][a] = np.array(points[g][a])\n",
    "        print(f'(G{g}, A{a}): {points[g][a].shape[0]} points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec001de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "a = 0\n",
    "for g in range(3):\n",
    "    r = grid_ratios[g]\n",
    "    # x1,  = grid_ratios[0], 0\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "    ax.set_xticks(np.arange(0, 1+r, r))\n",
    "    ax.set_yticks(np.arange(0, 1+r, r))\n",
    "    # ax.set_xticklabels([0, 1])\n",
    "    ax.set_yticklabels([round(1-j, 2) for j in ax.get_yticks()])\n",
    "    ax.set_xlim([-3*r, 1+3*r])\n",
    "    ax.set_ylim([-3*r, 1+3*r])\n",
    "    # ax.set_xlim([-3*r, 0.1])\n",
    "    # ax.set_ylim([1-(-3*r), 1-0.1])\n",
    "    plt.plot(points[g][a][:,0], 1-points[g][a][:,1], linestyle=' ', marker=\"x\", markersize=5)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    fig.autofmt_xdate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
