{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc40d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Train a YOLOv5 model on a custom dataset\n",
    "\n",
    "Usage:\n",
    "    $ python path/to/train.py --data coco128.yaml --weights yolov5s.pt --img 640\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from train_with_bcc import convert_yolo2bcc, nn_predict, convert_cs_yolo2bcc\n",
    "# YOLOBCC\n",
    "from train_with_bcc import read_crowdsourced_labels, init_bcc_params, \\\n",
    "    init_nn_output, compute_param_confusion_matrices, init_metrics, update_bcc_metrics\n",
    "from lib.BCCNet.VariationalInference.VB_iteration_yolo import VB_iteration as VBi_yolo\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FILE = Path(__file__).absolute()\n",
    "# sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path\n",
    "\n",
    "import val  # for end-of-epoch mAP\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Model\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
    "    strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\n",
    "    check_requirements, print_mutation, set_logging, one_cycle, colorstr, methods\n",
    "from utils.downloads import attempt_download\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.plots import plot_labels, plot_evolve\n",
    "from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, intersect_dicts, select_device, \\\n",
    "    torch_distributed_zero_first\n",
    "from utils.loggers.wandb.wandb_utils import check_wandb_resume\n",
    "from utils.metrics import fitness\n",
    "from utils.loggers import Loggers\n",
    "from utils.callbacks import Callbacks\n",
    "\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n",
    "RANK = int(os.getenv('RANK', -1))\n",
    "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6caefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_opt(known=False):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--bcc', action='store_true', help='Whether to run YOLO with or without BCC.')\n",
    "    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')\n",
    "    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n",
    "    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='dataset.yaml path')\n",
    "    parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch.yaml', help='hyperparameters path')\n",
    "    parser.add_argument('--epochs', type=int, default=300)\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\n",
    "    parser.add_argument('--rect', action='store_true', help='rectangular training')\n",
    "    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')\n",
    "    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n",
    "    parser.add_argument('--noval', action='store_true', help='only validate final epoch')\n",
    "    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n",
    "    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')\n",
    "    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n",
    "    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in \"ram\" (default) or \"disk\"')\n",
    "    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\n",
    "    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')\n",
    "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\n",
    "    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\n",
    "    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')\n",
    "    parser.add_argument('--project', default='runs/train', help='save to project/name')\n",
    "    parser.add_argument('--entity', default=None, help='W&B entity')\n",
    "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--quad', action='store_true', help='quad dataloader')\n",
    "    parser.add_argument('--linear-lr', action='store_true', help='linear LR')\n",
    "    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')\n",
    "    parser.add_argument('--upload_dataset', action='store_true', help='Upload dataset as W&B artifact table')\n",
    "    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval for W&B')\n",
    "    parser.add_argument('--save_period', type=int, default=-1, help='Log model after every \"save_period\" epoch')\n",
    "    parser.add_argument('--artifact_alias', type=str, default=\"latest\", help='version of dataset artifact to be used')\n",
    "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\n",
    "    parser.add_argument('--freeze', type=int, default=0, help='Number of layers to freeze. backbone=10, all=24')\n",
    "    parser.add_argument('--patience', type=int, default=30, help='EarlyStopping patience (epochs)')\n",
    "    opt = parser.parse_known_args()[0] if known else parser.parse_args(\"\")\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab75535",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ec8153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 85c7316 torch 1.9.0 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mbcc=False, weights=yolov5s.pt, cfg=, data=data/toy.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=300, batch_size=2, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=0, project=runs/train, entity=None, name=exp, exist_ok=True, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=30\n"
     ]
    }
   ],
   "source": [
    "opt = parse_opt()\n",
    "opt.data = 'data/toy.yaml'\n",
    "opt.exist_ok = True\n",
    "opt.cache = None\n",
    "opt.workers = 0\n",
    "opt.batch_size = 2 # Change this to number of train images\n",
    "\n",
    "# Checks\n",
    "set_logging(RANK)\n",
    "if RANK in [-1, 0]:\n",
    "    print(colorstr('train: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\n",
    "#     check_git_status()\n",
    "#     check_requirements(requirements=FILE.parent / 'requirements.txt', exclude=['thop'])\n",
    "\n",
    "# Resume\n",
    "if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # resume an interrupted run\n",
    "    ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\n",
    "    assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\n",
    "    with open(Path(ckpt).parent.parent / 'opt.yaml') as f:\n",
    "        opt = argparse.Namespace(**yaml.safe_load(f))  # replace\n",
    "    opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate\n",
    "    LOGGER.info(f'Resuming training from {ckpt}')\n",
    "else:\n",
    "    opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check files\n",
    "    assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\n",
    "    if opt.evolve:\n",
    "        opt.project = 'runs/evolve'\n",
    "        opt.exist_ok = opt.resume\n",
    "    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))\n",
    "\n",
    "# DDP mode\n",
    "device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "if LOCAL_RANK != -1:\n",
    "    from datetime import timedelta\n",
    "    assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'\n",
    "    assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'\n",
    "    assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'\n",
    "    assert not opt.evolve, '--evolve argument is not compatible with DDP training'\n",
    "    torch.cuda.set_device(LOCAL_RANK)\n",
    "    device = torch.device('cuda', LOCAL_RANK)\n",
    "    dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\")\n",
    "hyp = opt.hyp\n",
    "callbacks=Callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a9801",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b14d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7066239 parameters, 7066239 gradients, 16.4 GFLOPs\n",
      "\n",
      "Transferred 356/362 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../../datasets/toy/labels/train.cache' images and labels... 2 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../../datasets/toy/labels/val.cache' images and labels... 2 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.25, Best Possible Recall (BPR) = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "nn_predict() missing 1 required positional argument: 'imgsz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ff262d67d1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mpred_yolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mpred_bcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo2bcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_yolo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# pred_bcc = convert_yolo2bcc(pred_yolo.cpu().detach().numpy(), n_anchor_choices, nc, grid_ratios, intermediate_yolo_mode=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: nn_predict() missing 1 required positional argument: 'imgsz'"
     ]
    }
   ],
   "source": [
    "save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \\\n",
    "    Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n",
    "    opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n",
    "\n",
    "# Directories\n",
    "w = save_dir / 'weights'  # weights dir\n",
    "w.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "last, best = w / 'last.pt', w / 'best.pt'\n",
    "\n",
    "# Hyperparameters\n",
    "if isinstance(hyp, str):\n",
    "    with open(hyp) as f:\n",
    "        hyp = yaml.safe_load(f)  # load hyps dict\n",
    "LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\n",
    "\n",
    "# Save run settings\n",
    "with open(save_dir / 'hyp.yaml', 'w') as f:\n",
    "    yaml.safe_dump(hyp, f, sort_keys=False)\n",
    "with open(save_dir / 'opt.yaml', 'w') as f:\n",
    "    yaml.safe_dump(vars(opt), f, sort_keys=False)\n",
    "data_dict = {\n",
    "    'path': '../../datasets/toy',  # dataset root dir\n",
    "    'train': 'images/train',  # train images (relative to 'path') 128 images\n",
    "    'val': 'images/val',  # val images (relative to 'path') 128 images\n",
    "    'test': 'images/test', # test images (optional)\n",
    "    'nc': 2,  # number of classes\n",
    "    'names': ['bone-loss', 'dental-caries']  # class names\n",
    "}\n",
    "for x in ['train', 'val', 'test']:\n",
    "    data_dict[x] = os.path.join(data_dict['path'], data_dict[x])\n",
    "\n",
    "# Loggers\n",
    "if RANK in [-1, 0]:\n",
    "    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n",
    "    if loggers.wandb:\n",
    "        data_dict = loggers.wandb.data_dict\n",
    "        if resume:\n",
    "            weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp\n",
    "\n",
    "    # Register actions\n",
    "    for k in methods(loggers):\n",
    "        callbacks.register_action(k, callback=getattr(loggers, k))\n",
    "\n",
    "# Config\n",
    "plots = not evolve  # create plots\n",
    "cuda = device.type != 'cpu'\n",
    "init_seeds(1 + RANK)\n",
    "with torch_distributed_zero_first(RANK):\n",
    "    data_dict = data_dict or check_dataset(data)  # check if None\n",
    "train_path, val_path = data_dict['train'], data_dict['val']\n",
    "nc = 1 if single_cls else int(data_dict['nc'])  # number of classes\n",
    "names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
    "assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check\n",
    "is_coco = data.endswith('coco.yaml') and nc == 80  # COCO dataset\n",
    "\n",
    "# Model\n",
    "pretrained = weights.endswith('.pt')\n",
    "if pretrained:\n",
    "    with torch_distributed_zero_first(RANK):\n",
    "        weights = attempt_download(weights)  # download if not found locally\n",
    "    ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "    model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "    exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys\n",
    "    csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "    csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n",
    "    model.load_state_dict(csd, strict=False)  # load\n",
    "    LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report\n",
    "else:\n",
    "    model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "\n",
    "# Freeze\n",
    "freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = True  # train all layers\n",
    "    if any(x in k for x in freeze):\n",
    "        print(f'freezing {k}')\n",
    "        v.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "nbs = 64  # nominal batch size\n",
    "accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
    "hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
    "LOGGER.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\n",
    "\n",
    "g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "for v in model.modules():\n",
    "    if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\n",
    "        g2.append(v.bias)\n",
    "    if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\n",
    "        g0.append(v.weight)\n",
    "    elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n",
    "        g1.append(v.weight)\n",
    "\n",
    "if opt.adam:\n",
    "    optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "else:\n",
    "    optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "\n",
    "optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "optimizer.add_param_group({'params': g2})  # add g2 (biases)\n",
    "LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups \"\n",
    "            f\"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias\")\n",
    "del g0, g1, g2\n",
    "\n",
    "# Scheduler\n",
    "if opt.linear_lr:\n",
    "    lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\n",
    "else:\n",
    "    lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
    "\n",
    "# EMA\n",
    "ema = ModelEMA(model) if RANK in [-1, 0] else None\n",
    "\n",
    "# Resume\n",
    "start_epoch, best_fitness = 0, 0.0\n",
    "if pretrained:\n",
    "    # Optimizer\n",
    "    if ckpt['optimizer'] is not None:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        best_fitness = ckpt['best_fitness']\n",
    "\n",
    "    # EMA\n",
    "    if ema and ckpt.get('ema'):\n",
    "        ema.ema.load_state_dict(ckpt['ema'].float().state_dict())\n",
    "        ema.updates = ckpt['updates']\n",
    "\n",
    "    # Epochs\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    if resume:\n",
    "        assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'\n",
    "    if epochs < start_epoch:\n",
    "        LOGGER.info(f\"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.\")\n",
    "        epochs += ckpt['epoch']  # finetune additional epochs\n",
    "\n",
    "    del ckpt, csd\n",
    "\n",
    "# Image sizes\n",
    "gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
    "imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
    "\n",
    "# DP mode\n",
    "if cuda and RANK == -1 and torch.cuda.device_count() > 1:\n",
    "    logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'\n",
    "                    'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# SyncBatchNorm\n",
    "if opt.sync_bn and cuda and RANK != -1:\n",
    "    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "    LOGGER.info('Using SyncBatchNorm()')\n",
    "\n",
    "# Trainloader\n",
    "train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
    "                                          hyp=hyp, augment=False, cache=opt.cache, rect=opt.rect, rank=RANK,\n",
    "                                          workers=workers, image_weights=opt.image_weights, quad=opt.quad,\n",
    "                                          prefix=colorstr('train: '))\n",
    "# YOLOBCC\n",
    "# TODO: Verify the following (is nl == ng?):\n",
    "n_grid_choices, n_anchor_choices = model.model[-1].nl, model.model[-1].na\n",
    "grid_ratios = model.model[-1].stride.cpu().detach().numpy() / imgsz\n",
    "cstargets_all = read_crowdsourced_labels(data_dict)\n",
    "cstargets_all_bcc = convert_cs_yolo2bcc(cstargets_all, n_anchor_choices, nc, grid_ratios)\n",
    "cstargets = cstargets_all['train']\n",
    "cstargets_bcc = cstargets_all_bcc['train']\n",
    "mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class\n",
    "nb = len(train_loader)  # number of batches\n",
    "assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'\n",
    "\n",
    "# Process 0\n",
    "if RANK in [-1, 0]:\n",
    "    val_loader, val_dataset = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,\n",
    "                                   hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,\n",
    "                                   workers=workers, pad=0.5,\n",
    "                                   prefix=colorstr('val: '))\n",
    "\n",
    "    if not resume:\n",
    "        labels = np.concatenate(dataset.labels, 0)\n",
    "        # c = torch.tensor(labels[:, 0])  # classes\n",
    "        # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\n",
    "        # model._initialize_biases(cf.to(device))\n",
    "        if plots:\n",
    "            plot_labels(labels, names, save_dir)\n",
    "\n",
    "        # Anchors\n",
    "        if not opt.noautoanchor:\n",
    "            check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
    "        model.half().float()  # pre-reduce anchor precision\n",
    "\n",
    "    callbacks.on_pretrain_routine_end()\n",
    "\n",
    "# DDP mode\n",
    "if cuda and RANK != -1:\n",
    "    model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\n",
    "\n",
    "# Model parameters\n",
    "hyp['box'] *= 3. / nl  # scale to layers\n",
    "hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layers\n",
    "hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layers\n",
    "hyp['label_smoothing'] = opt.label_smoothing\n",
    "model.nc = nc  # attach number of classes to model\n",
    "model.hyp = hyp  # attach hyperparameters to model\n",
    "model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n",
    "model.names = names\n",
    "\n",
    "# Start training\n",
    "t0 = time.time()\n",
    "nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "# nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
    "last_opt_step = -1\n",
    "maps = np.zeros(nc)  # mAP per class\n",
    "results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "scaler = amp.GradScaler(enabled=cuda)\n",
    "stopper = EarlyStopping(patience=opt.patience)\n",
    "compute_loss = ComputeLoss(model)  # init loss class\n",
    "# YOLOBCC\n",
    "bcc_params = init_bcc_params()\n",
    "bcc_params['n_epoch'] = epochs\n",
    "pcm = compute_param_confusion_matrices(bcc_params)\n",
    "pred0_bcc = init_nn_output(dataset.n, grid_ratios, n_anchor_choices, bcc_params)\n",
    "bcc_metrics = init_metrics(bcc_params['n_epoch'])\n",
    "LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\\n'\n",
    "            f'Using {train_loader.num_workers} dataloader workers\\n'\n",
    "            f\"Logging results to {colorstr('bold', save_dir)}\\n\"\n",
    "            f'Starting training for {epochs} epochs...')\n",
    "for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "    model.train()\n",
    "    if epoch == start_epoch:\n",
    "        pred_bcc = pred0_bcc\n",
    "\n",
    "    # Update image weights (optional, single-GPU only)\n",
    "    if opt.image_weights:\n",
    "        cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\n",
    "        iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\n",
    "        dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\n",
    "\n",
    "    # Update mosaic border (optional)\n",
    "    # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "    # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "    # GOVIND0: Find mean losses by comparing pred with `qtargets`, not `targets`. Look for pointer GOVIND1\n",
    "    mloss = torch.zeros(3, device=device)  # mean losses\n",
    "    if RANK != -1:\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "    pbar = enumerate(train_loader)\n",
    "    LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "    if RANK in [-1, 0]:\n",
    "        pbar = tqdm(pbar, total=nb)  # progress bar\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # YOLOBCC\n",
    "    qtargets, pcm['variational'], lb = VBi_yolo(cstargets_bcc, pred_bcc, pcm['variational'], pcm['prior'])\n",
    "    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "        # YOLOBCC\n",
    "        batch_qtargets = qtargets\n",
    "        ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "        imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "        # Warmup\n",
    "        if ni <= nw:\n",
    "            xi = [0, nw]  # x interp\n",
    "            # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "            accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "            for j, x in enumerate(optimizer.param_groups):\n",
    "                # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                if 'momentum' in x:\n",
    "                    x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "        # Multi-scale\n",
    "        if opt.multi_scale:\n",
    "            sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "            sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "            if sf != 1:\n",
    "                ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Forward\n",
    "        with amp.autocast(enabled=cuda):\n",
    "            model.eval()\n",
    "            pred_yolo = nn_predict(model, imgs)\n",
    "            pred_bcc = yolo2bcc(pred_yolo)\n",
    "            # pred_bcc = convert_yolo2bcc(pred_yolo.cpu().detach().numpy(), n_anchor_choices, nc, grid_ratios, intermediate_yolo_mode=True)\n",
    "            model.train()\n",
    "            pred = model(imgs)  # forward\n",
    "            # GOVIND1: Don't compare w.r.t. targets, but w.r.t. qtargets (generated by BCC)\n",
    "            # YOLOBCC\n",
    "            loss, loss_items = compute_loss(pred, batch_qtargets)  # loss scaled by batch_size\n",
    "            if RANK != -1:\n",
    "                loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
    "            if opt.quad:\n",
    "                loss *= 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd80e75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 25200, 7]),\n",
       " tensor([[[3.62112e-03, 9.27925e-03, 1.36846e+01,  ..., 1.49550e-03, 8.22292e-01, 5.17919e-02],\n",
       "          [1.59202e-02, 7.25159e-03, 1.29100e+01,  ..., 1.89948e-03, 8.26544e-01, 6.36292e-02],\n",
       "          [2.91756e-02, 7.79393e-03, 1.06776e+01,  ..., 1.34058e-03, 7.16163e-01, 6.48550e-02],\n",
       "          ...,\n",
       "          [8.87906e-01, 9.82400e-01, 3.69486e+02,  ..., 1.89169e-02, 3.72366e-01, 5.01492e-01],\n",
       "          [9.36504e-01, 9.79430e-01, 3.28244e+02,  ..., 1.73013e-02, 3.57602e-01, 4.57227e-01],\n",
       "          [9.83267e-01, 9.79760e-01, 3.76296e+02,  ..., 1.72529e-02, 3.72640e-01, 4.56131e-01]],\n",
       " \n",
       "         [[3.80318e-03, 8.85521e-03, 1.23594e+01,  ..., 1.40285e-03, 8.24831e-01, 5.40825e-02],\n",
       "          [1.61285e-02, 6.41171e-03, 1.08387e+01,  ..., 1.87619e-03, 8.26284e-01, 6.93943e-02],\n",
       "          [2.98106e-02, 6.88588e-03, 8.18122e+00,  ..., 1.46679e-03, 7.17260e-01, 6.90101e-02],\n",
       "          ...,\n",
       "          [8.84591e-01, 9.75536e-01, 3.74005e+02,  ..., 1.49167e-02, 4.05465e-01, 5.50655e-01],\n",
       "          [9.33291e-01, 9.73586e-01, 3.40639e+02,  ..., 1.48356e-02, 4.04855e-01, 5.14176e-01],\n",
       "          [9.83699e-01, 9.76865e-01, 3.97344e+02,  ..., 1.61515e-02, 4.06849e-01, 5.15116e-01]]], grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred_yolo = nn_predict(model, imgs, imgsz, transform_format_flag=False)\n",
    "pred_yolo.shape, pred_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1da436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_converter import yolo2bcc_new\n",
    "pred_bcc, pred_yolo_wh = yolo2bcc_new(pred_yolo)\n",
    "model.train()\n",
    "pred_yolo_wh = pred_yolo_wh.cpu().detach().numpy()\n",
    "pred = model(imgs)  # forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5a05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtargets, pcm_v, lb = VBi_yolo(cstargets_bcc, pred_bcc.detach().numpy(), pcm['variational'], pcm['prior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a9a8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 25200, 2), (2, 25200, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtargets.shape, pred_yolo_wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a9ce266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7167199148029819 0.2063380281690141 0.10862619808306709 0.18450704225352113\r\n",
      "1 0.6629392971246006 0.6330985915492958 0.10543130990415335 0.1732394366197183\r\n",
      "0 0.1847710330138445 0.6098591549295774 0.13099041533546327 0.15774647887323945\r\n",
      "1 0.5213219616204691 0.30277777777777776 0.31544562899786777 0.41095555555555563\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../../datasets/toy/labels/train/train*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d67c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6]),\n",
       " tensor([[0.00000, 1.00000, 0.52132, 0.34869, 0.31545, 0.31528],\n",
       "         [1.00000, 1.00000, 0.71672, 0.27838, 0.10863, 0.13925],\n",
       "         [1.00000, 1.00000, 0.66294, 0.60045, 0.10543, 0.13074],\n",
       "         [1.00000, 0.00000, 0.18477, 0.58291, 0.13099, 0.11905]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ecd643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          0,           0,     0.00625,     0.00625,      13.685,      3.3365],\n",
       "       [          0,           0,     0.01875,     0.00625,       12.91,      10.012],\n",
       "       [          0,           0,     0.03125,     0.00625,      10.678,      7.7009],\n",
       "       ...,\n",
       "       [          1,           1,       0.875,       0.975,      374.01,      373.05],\n",
       "       [          1,           1,       0.925,       0.975,      340.64,      343.07],\n",
       "       [          1,           1,       0.975,       0.975,      397.34,      352.22]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from label_converter import find_grid_center\n",
    "def qt2yolo(qt):\n",
    "    G = grid_ratios\n",
    "    Ng = G.shape[0]\n",
    "    Na = 3\n",
    "    y_bcc = []\n",
    "    num_images, num_grid_cells, _ = qt.shape\n",
    "    for i in range(num_images):\n",
    "        effective_id = 0\n",
    "        cs = np.argmax(qt[i, :], 1)            \n",
    "        for g in range(Ng):\n",
    "            g_frac = G[g]\n",
    "            S_g = np.ceil(1/g_frac).astype(int)\n",
    "            for a in range(Na):\n",
    "                for gc in range(S_g*S_g):\n",
    "                    x, y = find_grid_center(g_frac, g_frac, gc)\n",
    "                    w, h = pred_yolo_wh[i][effective_id]\n",
    "                    c = cs[effective_id]\n",
    "    #                 if (effective_id%100==0):\n",
    "    #                     print('I =', i, ', g =', g, ', a =', a, ', gc =', gc, qt[i, effective_id, :], \\\n",
    "    #                           (round(x, 2), round(y, 2), round(w, 2), round(h, 2), c))\n",
    "                    y_bcc.append([i, c, x, y, w, h])\n",
    "                    effective_id += 1\n",
    "    return np.array(y_bcc)\n",
    "\n",
    "qt2yolo(qtargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a454c4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[    0.97874,      2.2409],\n",
       "        [     1.8676,    -0.97728],\n",
       "        [    0.95009,    -0.15136],\n",
       "        ...,\n",
       "        [    0.79987,       1.579],\n",
       "        [   -0.47848,     -1.3616],\n",
       "        [    -1.1311,    -0.87371]],\n",
       "\n",
       "       [[    0.45091,    -0.99932],\n",
       "        [    -1.9487,     -1.0684],\n",
       "        [   -0.68979,     0.19958],\n",
       "        ...,\n",
       "        [    0.14034,    0.040687],\n",
       "        [    0.56578,     -2.6195],\n",
       "        [   -0.79674,    -0.92146]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred0_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e742b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "pil_img = Image(filename=os.path.join(data_dict['train'], 'train1.jpg'))\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ffd85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85817c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = targets[:,2:]\n",
    "# z[:, :2] = z[:, :2] - z[:, 2:]/2\n",
    "# z[:, 2:] = z[:, :2] + z[:, 2:]\n",
    "z*x_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images, _, x_size, y_size = imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc = torch.log(pred_yolo[:, ..., 5:]/pred_yolo[:,...,5:].sum(2).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yolo[:,...,5:].sum(2).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d50923",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_qtargets.shape, batch_qtarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec089fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_bcc2yolo():\n",
    "bcc_labels = {'labels': batch_qtargets, 'Na': 3, 'Nc': 2, 'wh_map': }\n",
    "G, Na, Nc, wh_map = [bcc_labels[x] for x in ['G', 'Na', 'Nc', 'wh_map']]\n",
    "Ng = G.shape[0]\n",
    "S = (1/G).astype(int)\n",
    "C = list(range(Nc))\n",
    "effective_id = 0\n",
    "yolo_labels = []\n",
    "for g in range(Ng):\n",
    "    g_labels = []\n",
    "    g_frac = G[g]\n",
    "    cells_per_side = np.ceil(1/g_frac).astype(int)\n",
    "    for a in range(Na):\n",
    "        a_labels = []\n",
    "        for gc in range(cells_per_side**2):  # grid-cells\n",
    "            c = bcc_labels['labels'][effective_id]\n",
    "            effective_id += 1\n",
    "            if c == BACKGROUND_CLASS_ID:\n",
    "                continue\n",
    "            x, y = find_grid_center(g_frac, g_frac, gc)\n",
    "            # print(effective_id, c)\n",
    "            w, h = bcc_labels['wh_map'][(g, a, gc)]\n",
    "            gc_label = [c, x, y, w, h]\n",
    "            a_labels.append(gc_label)\n",
    "        g_labels.append(a_labels)\n",
    "    yolo_labels.append(g_labels)\n",
    "if return_flattened:\n",
    "    y = np.concatenate([np.concatenate(x) for x in yolo_labels])\n",
    "else:\n",
    "    y = np.array(yolo_labels)\n",
    "return {'labels': y, 'G': G, 'Nc': Nc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5279b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "imgplot = plt.imshow(np.transpose(imgs[1], (1, 2, 0)), cmap='gray')\n",
    "\n",
    "for i in range(3, z.shape[0]):\n",
    "    x, y, w, h = z[i]*x_size\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "imgplot = plt.imshow(np.transpose(imgs[0], (1, 2, 0)), cmap='gray')\n",
    "\n",
    "for i in range(3):\n",
    "    x, y, w, h = z[i]*x_size\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_t, pcm_var, lb = VBi_yolo(cstargets_bcc, pred_bcc.cpu().detach().numpy(), pcm['variational'], pcm['prior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z[:,...,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(cstargets_bcc[1][:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bcc = convert_yolo2bcc(pred_yolo.cpu().detach().numpy(), n_anchor_choices, nc, grid_ratios, intermediate_yolo_mode = True)\n",
    "pred_bcc.shape, pred_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e42192",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yolo.shape, pred_bcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(pred_yolo[...,0].cpu().detach().numpy()[1]), Counter(pred_bcc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(pred_yolo[0][:, 0].cpu().detach().numpy()), Counter(pred_bcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yolo[0, :4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pred_yolo.cpu().detach().numpy()\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2185bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d37dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "effective_id = 0\n",
    "points = {}\n",
    "for g, gr in enumerate(grid_ratios):\n",
    "    points[g] = {}\n",
    "    cells_per_side = np.ceil(1/gr).astype(int)\n",
    "    for a in range(3):\n",
    "        points[g][a] = []\n",
    "        for gc in range(cells_per_side**2):\n",
    "            points[g][a].append(y[i, effective_id, [1,2]])\n",
    "            effective_id += 1\n",
    "        points[g][a] = np.array(points[g][a])\n",
    "        print(f'(G{g}, A{a}): {points[g][a].shape[0]} points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec001de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "a = 0\n",
    "for g in range(3):\n",
    "    r = grid_ratios[g]\n",
    "    # x1,  = grid_ratios[0], 0\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "    ax.set_xticks(np.arange(0, 1+r, r))\n",
    "    ax.set_yticks(np.arange(0, 1+r, r))\n",
    "    # ax.set_xticklabels([0, 1])\n",
    "    ax.set_yticklabels([round(1-j, 2) for j in ax.get_yticks()])\n",
    "    ax.set_xlim([-3*r, 1+3*r])\n",
    "    ax.set_ylim([-3*r, 1+3*r])\n",
    "    # ax.set_xlim([-3*r, 0.1])\n",
    "    # ax.set_ylim([1-(-3*r), 1-0.1])\n",
    "    plt.plot(points[g][a][:,0], 1-points[g][a][:,1], linestyle=' ', marker=\"x\", markersize=5)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    fig.autofmt_xdate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
