{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a11c619",
   "metadata": {},
   "source": [
    "# label_converter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f392a3a",
   "metadata": {},
   "source": [
    "list of in-use functions:\n",
    "1. yolo2bcc_newer(y_yolo, imgsz, silent = True)\n",
    "2. qt2yolo_optimized(qt, G, Na, wh_yolo, torchMode=False, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ee2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKGROUND_CLASS_ID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo2bcc_newer(y_yolo, imgsz, silent = True):\n",
    "    wh = y_yolo[:, ..., 2:4]/imgsz\n",
    "    conf = y_yolo[:, ..., 4]\n",
    "\n",
    "    p = y_yolo[..., 4]\n",
    "    sigma_t = y_yolo[..., 5:]\n",
    "    sigma_prime_t = sigma_t / sigma_t.sum(axis=2).unsqueeze(-1)\n",
    "    class_prob = sigma_prime_t*p.unsqueeze(-1)\n",
    "    bkgd_prob = 1-p\n",
    "    bcc_prob = torch.cat([class_prob, bkgd_prob.unsqueeze(-1)], -1)\n",
    "    if not silent:\n",
    "        mins = [round(x, 6) for x in list(bcc_prob.min(1).values.min(0).values.cpu().detach().numpy())]\n",
    "        maxs = [round(x, 6) for x in list(bcc_prob.max(1).values.max(0).values.cpu().detach().numpy())]\n",
    "        print('Minimum probs (c1, c2, bkgd):', mins)\n",
    "        print('Maximum probs (c1, c2, bkgd):', maxs)\n",
    "    bcc_logits = torch.log(bcc_prob)\n",
    "    return bcc_logits, wh, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc60275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qt2yolo_optimized(qt, G, Na, wh_yolo, torchMode=False, device=None):\n",
    "    Ng = G.shape[0]\n",
    "    num_images = qt.shape[0]\n",
    "    y_bcc = []\n",
    "    for i in range(num_images):\n",
    "        cs = torch.argmax(qt[i, :], 1).to(device)\n",
    "        st = 0\n",
    "        for g in range(Ng):\n",
    "            g_frac = G[g]\n",
    "            S_g = np.ceil(1/g_frac).astype(int)\n",
    "            n_cells = S_g*S_g\n",
    "            for a in range(Na):\n",
    "                z = torch.linspace(g_frac/2, 1-g_frac/2, S_g).repeat(S_g, 1).unsqueeze(-1)\n",
    "                xy = torch.cat((z.permute(1, 0, 2), z), 2).permute(1, 0, 2).reshape(n_cells, 2).to(device)\n",
    "                wh = wh_yolo[i][st:st+n_cells]\n",
    "                c = cs[st:st+n_cells]\n",
    "                icxywh = torch.cat(((i*torch.ones(n_cells, 1)).to(device), c.unsqueeze(-1), xy, wh), 1)\n",
    "                y_bcc.append(icxywh)\n",
    "                st += n_cells\n",
    "    return torch.cat(y_bcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f738a",
   "metadata": {},
   "source": [
    "# train_with_bcc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da470fdf",
   "metadata": {},
   "source": [
    "list of in-use functions:\n",
    "1. get_file_volunteers_dict(data_dict, mode='train', vol_id_map=VOL_ID_MAP)\n",
    "2. compute_param_confusion_matrices(bcc_params, torchMode=False)\n",
    "3. init_bcc_params(K=4)\n",
    "4. init_metrics(n_epochs)\n",
    "5. convert_target_volunteers_yolo2bcc(target_volunteers, Na=3, Nc=2, G=DEFAULT_G, batch_size=None, vol_id_map=VOL_ID_MAP)\n",
    "6. xywhpc1ck_to_cxywh(y)\n",
    "7. nn_predict(model, x, imgsz, offgrid_translate_flag=True, normalize_flag=True, transform_format_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b41af16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_G = np.array([1.0/32, 1.0/16, 1.0/8])\n",
    "VOL_ID_MAP = {'Camellia': 0, 'Conghui': 1, 'HaoWen': 2, 'Xiongjie': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b66fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_volunteers_dict(data_dict, mode='train', vol_id_map=VOL_ID_MAP):\n",
    "    vol_path = os.path.join(data_dict['path'], 'volunteers')\n",
    "    vol_mode_path = os.path.join(vol_path, mode)\n",
    "    file_names = [x for x in os.listdir(vol_mode_path) if not x.startswith('.')]\n",
    "    file_vols_dict = {}\n",
    "    for fn in file_names:\n",
    "        vol_file_path = os.path.join(vol_mode_path, fn)\n",
    "        with open(vol_file_path) as f:\n",
    "            vol_seq = [vol_id_map[x.strip()] for x in f.readlines()]\n",
    "        file_vols_dict[fn] = torch.tensor(vol_seq).int()\n",
    "    return file_vols_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abee805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_param_confusion_matrices(bcc_params, torchMode=False):\n",
    "    # set up variational parameters\n",
    "    prior_param_confusion_matrices = confusion_matrix.initialise_prior(n_classes=bcc_params['n_classes'],\n",
    "                                                                       n_volunteers=bcc_params['n_crowd_members'],\n",
    "                                                                       alpha_diag_prior=bcc_params['confusion_matrix_diagonal_prior'],\n",
    "                                                                       torchMode = torchMode)\n",
    "    variational_param_confusion_matrices = prior_param_confusion_matrices.detach().clone() if torchMode else np.copy(prior_param_confusion_matrices)\n",
    "    return {'prior': prior_param_confusion_matrices, 'variational': variational_param_confusion_matrices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bcc_params(K=4):\n",
    "    bcc_params = {'n_classes': 3,\n",
    "                  'n_crowd_members': K,\n",
    "                  'confusion_matrix_diagonal_prior': 1e-1,\n",
    "                  'convergence_threshold': 1e-6}\n",
    "    return bcc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787478d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metrics(n_epochs):\n",
    "    metrics = {'accuracy': np.zeros((n_epochs,), dtype=np.float64)}\n",
    "    return {'train': metrics, 'test': metrics, 'posterior_estimate': metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_target_volunteers_yolo2bcc(target_volunteers, Na=3, Nc=2, G=DEFAULT_G, batch_size=None, vol_id_map=VOL_ID_MAP):\n",
    "    n_images = batch_size\n",
    "    n_vols = len(vol_id_map)\n",
    "    Ng = G.shape[0]\n",
    "\n",
    "    targets_per_i_bcc_list = []\n",
    "    for i in range(n_images):\n",
    "        target_vols_per_i = target_volunteers[target_volunteers[:, 0] == i][:, 1:]\n",
    "        targets_per_ig_bcc_list = []\n",
    "        for g in range(Ng):  # per grid choice\n",
    "            g_frac = G[g]\n",
    "            S_g = np.ceil(1/g_frac).astype(int)**2\n",
    "            # Don't need a loop for anchor-boxes as we are simply repeating Na times below.\n",
    "            targets_per_iv_bcc_list = []\n",
    "            for v in range(n_vols):\n",
    "                targets_per_iv = target_vols_per_i[target_vols_per_i[:, -1] == v][:, :-1]\n",
    "                c, x, y, _, _ = targets_per_iv.T # w and h are ignored\n",
    "\n",
    "                x_cell_ids = torch.where(x<1, x/g_frac, torch.ones(x.shape)*(np.ceil(1/g_frac))).int()\n",
    "                y_cell_ids = torch.where(y<1, y/g_frac, torch.ones(y.shape)*(np.ceil(1/g_frac))).int()\n",
    "                gc_ids = ((y_cell_ids)*(np.ceil(1/g_frac)) + x_cell_ids).long()\n",
    "\n",
    "                targets_per_iv_bcc = BACKGROUND_CLASS_ID * torch.ones(S_g)\n",
    "                targets_per_iv_bcc[gc_ids] = c\n",
    "                targets_per_iv_bcc_list.append(targets_per_iv_bcc)\n",
    "            targets_per_iga_bcc = torch.stack(tuple(targets_per_iv_bcc_list)).T\n",
    "            targets_per_ig_bcc = targets_per_iga_bcc.repeat((Na, 1))\n",
    "            targets_per_ig_bcc_list.append(targets_per_ig_bcc)\n",
    "        targets_per_i_bcc = torch.cat(targets_per_ig_bcc_list)\n",
    "        targets_per_i_bcc_list.append(targets_per_i_bcc)\n",
    "    target_volunteers_bcc = torch.stack(tuple(targets_per_i_bcc_list))\n",
    "    return target_volunteers_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywhpc1ck_to_cxywh(y):\n",
    "    y[:, 4] = y[:, 5:].max(dim=1).indices\n",
    "    y = y[:, :5]\n",
    "    y = y[:, [4, 0, 1, 2, 3]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0950abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(model, x, imgsz, offgrid_translate_flag=True, normalize_flag=True, transform_format_flag=True):\n",
    "    # update of approximating posterior for the true labels and confusion matrices\n",
    "    # get current predictions from a neural network\n",
    "    y = model(x)[0]\n",
    "    if offgrid_translate_flag:\n",
    "        # TODO: A quickfix here is to force-translate any point lying outside the grid to the nearest grid cell.\n",
    "        y[..., :2][y[..., :2] < 0] = 0\n",
    "        y[..., :2][y[..., :2] > imgsz] = imgsz\n",
    "    if normalize_flag:\n",
    "        y[..., :2] = y[..., :2] / imgsz\n",
    "    if transform_format_flag: # Transform (x, y, w, h, prob, c1, ..., ck) to (c, x, y, w, h)\n",
    "        z = y[..., :5]\n",
    "        n_images = y.shape[0]\n",
    "        for i in range(n_images):\n",
    "            z[i] = xywhpc1ck_to_cxywh(y[i])\n",
    "        y = z\n",
    "    return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
