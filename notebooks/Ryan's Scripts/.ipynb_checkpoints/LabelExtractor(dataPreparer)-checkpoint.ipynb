{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b20cfb7-5b59-4619-8151-ed80f1a256cc",
   "metadata": {},
   "source": [
    "## This code extracts labels from a zooniverse export and reformats it into the yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1607e5e-f374-495a-9192-49d55c4f34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import filecmp\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150cb624-d777-42f9-b19b-1f8b5e91eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re formats the coordanates for use with yolo v5\n",
    "# In:\n",
    "#      x2 (list) : x coordanates for all labels for a single image \n",
    "#      y2 (list) : y coordanates for all labels for a single image \n",
    "#      w2 (list) : width distances for all labels for a single image \n",
    "#      h2 (list) : height distances for all labels for a single image \n",
    "#      origW (list) : Original width of the target image\n",
    "#      origH (list) : Original height of the target image\n",
    "# Out: \n",
    "#      x3 (list) : x coordanates after yoloV5 preperation\n",
    "#      y3 (list) : y coordanates after yoloV5 preperation\n",
    "#      w3 (list) : Height distances after yoloV5 preperation\n",
    "#      h3 (list) : height distances after yoloV5 preperation\n",
    "def PrepForYolo (x,y,w,h,origW,origH):\n",
    "    x2 = []\n",
    "    y2 = []\n",
    "    w2 = []\n",
    "    h2 = []\n",
    "    for q in range(len(x)):\n",
    "        # moves anchor to the middle of the bounding box\n",
    "        x2.append(x[q]+(w[q]/2))\n",
    "        y2.append(y[q]+(h[q]/2))\n",
    "        # normalises the values\n",
    "        x2[q] = x2[q]/origW\n",
    "        y2[q] = y2[q]/origH\n",
    "        w2.append(w[q]/origW)\n",
    "        h2.append(h[q]/origH)\n",
    "    return(x2, y2, w2, h2)\n",
    "\n",
    "\n",
    "# This function creates the dataset for all volunteers. It creates the relivant file structures.\n",
    "# In:\n",
    "#      fmat (dictionary) : formatted bounding box information in yolo format for a single image\n",
    "#      ip (list) : the encrypted ip address of the annotators for each bounding box for a single image\n",
    "#      path (str) :  location of the intended save path \n",
    "#      pathType (str) : IIC or all users \n",
    "#      saveName (str) : name of the current image\n",
    "#      imgLoc (str) : location of the images \n",
    "# Out: \n",
    "#      None\n",
    "def SaveCrowd (fmat, ip, path, pathType, saveName, imgLoc):\n",
    "        \n",
    "    np.savetxt(path+'\\\\'+pathType+'\\\\labels\\\\train\\\\'+saveName+'.txt', fmat.values, fmt='%i %1.10f %1.10f %1.10f %1.10f')\n",
    "    np.savetxt(path+'\\\\'+pathType+'\\\\volunteers\\\\train\\\\'+saveName+'.txt', ip, fmt='%s')\n",
    "    \n",
    "    shutil.copyfile(imgLoc+'\\\\'+saveName+'.JPG' , path+'\\\\'+pathType+'\\\\images\\\\train\\\\'+saveName+'.JPG')\n",
    "    \n",
    "    # return(saveName+' Done')\n",
    "\n",
    "# This function creates the IID dataset. It creates the relivant file structures.\n",
    "# In:\n",
    "#      fmat (list) : formatted bounding box information in yolo format for a single image and annotator\n",
    "#      ip (str) : the encrypted ip address for a single annotators for a single image\n",
    "#      path (str) :  location of the intended save path \n",
    "#      pathType (str) : IIC or all users \n",
    "#      saveName (str) : name of the current image\n",
    "#      imgLoc (str) : location of the images \n",
    "# Out: \n",
    "#      None\n",
    "def SaveIID (fmat, ip, path, pathType, saveName, imgLoc):\n",
    "        \n",
    "    np.savetxt(path+'\\\\'+pathType+'\\\\labels\\\\train\\\\'+saveName+'.'+ip+'.txt', fmat.values, fmt='%i %1.6f %1.6f %1.6f %1.6f')\n",
    "    \n",
    "    shutil.copyfile(imgLoc+'\\\\'+saveName+'.JPG' , path+'\\\\'+pathType+'\\\\images\\\\train\\\\'+saveName+'.'+ip+'.JPG')\n",
    "    # return(None)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# This function handels the train val splitting\n",
    "def valSplit(TTs, path, pathType, isIID):\n",
    "    direc = os.listdir(path+'\\\\'+pathType+'\\\\images\\\\train')\n",
    "    count = len(direc)\n",
    "    valSplit = round(count * (1-TTs))\n",
    "    for i in range(valSplit):\n",
    "        name, extension= os.path.splitext(direc[i])\n",
    "\n",
    "        shutil.move(path+'\\\\'+pathType+'\\\\images\\\\train\\\\'+direc[i] , path+'\\\\'+pathType+'\\\\images\\\\val\\\\'+direc[i])\n",
    "        shutil.move(path+'\\\\'+pathType+'\\\\labels\\\\train\\\\'+name+'.txt' , path+'\\\\'+pathType+'\\\\labels\\\\val\\\\'+name+'.txt')\n",
    "        if isIID == False:\n",
    "            shutil.move(path+'\\\\'+pathType+'\\\\volunteers\\\\train\\\\'+name+'.txt' , path+'\\\\'+pathType+'\\\\volunteers\\\\val\\\\'+name+'.txt')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "# This function gets the location of the test dataset, and copys it into the correct format\n",
    "def injectTestDS(testLocImages, testLocLabels, trgtLoc, pathType, iid):\n",
    "    direc = os.listdir(testLocImages)\n",
    "    for i in direc:\n",
    "        name, extension= os.path.splitext(i)\n",
    "        shutil.copyfile(testLocImages+'\\\\'+i , trgtLoc+'\\\\'+pathType+'\\\\images\\\\test\\\\'+i)\n",
    "        shutil.copyfile(testLocLabels+'\\\\'+name+'.txt' , trgtLoc+'\\\\'+pathType+'\\\\labels\\\\test\\\\'+name+'.txt')\n",
    "        if iid == False:\n",
    "            label = open(testLocLabels+'\\\\'+name+'.txt', \"r\")\n",
    "            rows = len([f.strip() for f in label])\n",
    "            expert = []\n",
    "            for l in range(rows):\n",
    "                expert.append('expert')\n",
    "            np.savetxt(trgtLoc+'\\\\'+pathType+'\\\\volunteers\\\\test\\\\'+name+'.txt', expert, fmt='%s')\n",
    "            \n",
    "            \n",
    "\n",
    "# This function removes repeated images in the train dataset that are already in the test dataset. So the test dataset is fully unseen\n",
    "def removeRepeated(testLocImg, trainLoc, pathType):\n",
    "    delImg=[]\n",
    "    trainLoc2 = trainLoc+'\\\\'+pathType+'\\\\images\\\\train'\n",
    "    direcTest = os.listdir(testLocImg)\n",
    "    direcTrain = os.listdir(trainLoc2)\n",
    "    for tst in direcTest:\n",
    "        for trn in direcTrain:\n",
    "            name, extension= os.path.splitext(tst)\n",
    "            name2, extension2= os.path.splitext(trn)\n",
    "            if name in name2:\n",
    "                # Counts repeated images\n",
    "                delImg.append(name2)\n",
    "                # deletes image in the train dataset\n",
    "                os.remove(trainLoc2 + '\\\\' + trn)\n",
    "                # deletes label in the train dataset\n",
    "                os.remove(trainLoc + '\\\\' + pathType + '\\\\labels\\\\train\\\\' + name2 + '.txt')\n",
    "                # deletes volunteer information in the train dataset\n",
    "                try:\n",
    "                    os.remove(trainLoc + '\\\\' + pathType + '\\\\volunteers\\\\train\\\\' + name2 + '.txt')\n",
    "                except:\n",
    "                    pass\n",
    "    print(str(len(delImg)) + ' Repeated Images. They Have Been Deleted: ')\n",
    "    print(delImg)\n",
    "\n",
    "\n",
    "# this function ensures each bounding box is within the bounds of the image\n",
    "def fixOutOfBoundsBoxes(x, y, w, h):\n",
    "    for pos in range(len(x)):\n",
    "        # the bounding box extends past the right of the image\n",
    "        if (x[pos]+(w[pos]/2))>1:\n",
    "            w[pos] = w[pos]-((x[pos]+(w[pos]/2))-1)\n",
    "            x[pos] = 1-(w[pos]/2)\n",
    "        # the bounding box extends past the left of the image\n",
    "        elif (x[pos]-(w[pos]/2))<0:\n",
    "            w[pos] = w[pos]+(x[pos]-(w[pos]/2))\n",
    "            x[pos] = w[pos]/2\n",
    "        # the bounding box extends past the bottom of the image\n",
    "        if (y[pos]+(h[pos]/2))>1:\n",
    "            h[pos] = h[pos]-((y[pos]+(h[pos]/2))-1)\n",
    "            y[pos] = 1-(h[pos]/2)\n",
    "        # the bounding box extends past the top of the image\n",
    "        elif (y[pos]-(h[pos]/2))<0:\n",
    "            h[pos] = h[pos]+(y[pos]-(h[pos]/2))\n",
    "            y[pos] = h[pos]/2\n",
    "    return(x,y,w,h)\n",
    "\n",
    "# switches 2 labels of the users choice\n",
    "def switchLabel(swapLabel, label):\n",
    "    if label[\"value\"] == swapLabel[0]:\n",
    "        label[\"value\"] = swapLabel[1]\n",
    "    elif label[\"value\"] == swapLabel[1]:\n",
    "        label[\"value\"] = swapLabel[0]\n",
    "    return(label)\n",
    "        \n",
    "# # tests in turn A(anchor before bounds), A(anchor past bounds), C(anchor before bounds), C(anchor past bounds), B(anchor before bounds), B(anchor past bounds), D(anchor before bounds), D(anchor past bounds)\n",
    "# # then repeated for the corner examples (AB, CD, DA, BC) with anchor inside and outside of bounds\n",
    "# X1, Y1, W1, H1 = fixOutOfBoundsBoxes([0.95,1.1,0.002,-0.065,0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5,0.95,1.1,0.002,-0.065],[0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28],[0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24])\n",
    "# X2, Y2, W2, H2 = fixOutOfBoundsBoxes([0.95,1.1,0.002,-0.065,0.95,1.1,0.002,-0.065],[0.95,1.1,0.002,-0.065,0.002,-0.065,0.95,1.1],[0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28],[0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24])\n",
    "# print('test1 (A1,A2,C1,C2,B1,B2,D1,D2): ',X1,W1)\n",
    "# print('test1 (AB1,AB2,CD1,CD2,DA1,DA2,BC1,BC2): ',X2,W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9ec769-4cbe-4d7f-ba5c-203277ad6dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2029it [00:00, 3097.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVED UNWANTED IMAGES:  Unknown-X-20200928-091843-XIQ+LFYOKA+S-0-YunpengÔÇÖs iMac.JPG\n",
      "0 Repeated Images. They Have Been Deleted: \n",
      "[]\n",
      "0 Repeated Images. They Have Been Deleted: \n",
      "[]\n",
      "############ DONE ############\n"
     ]
    }
   ],
   "source": [
    "# this section extracts all images and labels from the zooniverse output csv\n",
    "\n",
    "\n",
    "##### File Locations #####\n",
    "\n",
    "# file path to zooniverse\n",
    "dataset = pd.read_csv (r'C:\\Users\\rb01243\\OneDrive - University of Surrey\\Desktop\\zoon data\\Labels\\dental-disease-labelling-easy-classifications.csv')\n",
    "# file path that contains the images\n",
    "imageLocation = r'C:\\Users\\rb01243\\OneDrive - University of Surrey\\Desktop\\zoon data\\Data'\n",
    "# The location you want to save the output to\n",
    "saveLoaction = r'C:\\Users\\rb01243\\OneDrive - University of Surrey\\Desktop\\zoon data\\Finished Data'\n",
    "# location of the test image dataset labels (compleeted by a single expert 'ground truth labels')\n",
    "testLocationImg = r'C:\\Users\\rb01243\\OneDrive - University of Surrey\\Documents\\GitHub\\miccai\\miccai_dental_disease\\data\\datasets\\master\\images'\n",
    "# location of the test label dataset labels (compleeted by a single expert 'ground truth labels')\n",
    "testLocationLbl = r'C:\\Users\\rb01243\\OneDrive - University of Surrey\\Documents\\GitHub\\miccai\\miccai_dental_disease\\data\\datasets\\master\\labels\\Jonathan'\n",
    "\n",
    "\n",
    "#expertTestData = r'test'\n",
    "\n",
    "#########################\n",
    "\n",
    "######## Params #########\n",
    "\n",
    "# type of prefix dental experts use\n",
    "dentPrefix = 'dnt_'     # 'dnt_'\n",
    "# the split ratio for train and val datasets (ground truth Train data is already in the correct format) (between 0.0 and 1 foat)\n",
    "trainValSplit = 0.80\n",
    "# if you want only expert labels change this filter to True\n",
    "filterExpert = False\n",
    "# minimum number of annotators per image\n",
    "minAnnotators = 3\n",
    "# True if you want to create an IID dataset as well as a crowdsourced dataset (IID: crowdsourced labels for a single image are split up into their own .txt files, to act as repeated ground truth labels)\n",
    "includeIID = True   # True\n",
    "# if you want to generate a test dataset using the \n",
    "includeTestDS = True\n",
    "# Parameter to choose the minimum number of classifications a volunteer has to complete\n",
    "minClss = 500\n",
    "# The minimum workflow version\n",
    "minWorkflowVersion = 20.00\n",
    "# user this to remove a label type from the dataset\n",
    "remveLbl = 0 #       -1 = none     0 = Calculus Plaque,   1 = Dental Caries,      2 = Bone Loss        This parameter will also shift all subsequent labels down by 1 to compensate of the removal of the value\n",
    "# swap label (done before label removal [remveLbl])\n",
    "swapLabel = [1,2] #    Leave empty to not swap any labels      ZOONIVERSE EPORT IS IN THE FORMAT OF (0 = CALCULUS, 1 = CARIES, 2 = BONE LOSS) [0,2] = 3 label format, [1,2] = 2 label format\n",
    "# specific images to remove\n",
    "imgRemove = ['Unknown-X-20200928-091843-XIQ+LFYOKA+S-0-YunpengÔÇÖs iMac.JPG']\n",
    "########################\n",
    "\n",
    "# Name of the folder storing the images\n",
    "pthType = 'All_Volunteers_Calc_Removed_testtesttest' # All_Volunteers   All_Volunteers_Calc_Removed_John_Test      All_Volunteers_Calc_Removed\n",
    "\n",
    "# filters expert labels only\n",
    "if filterExpert == True:\n",
    "    pthType = 'Expert_Volunteers'\n",
    "    tempDS = dataset\n",
    "    dataset = tempDS.loc[(tempDS['expert'] == 'expert')|(tempDS['gold_standard'] == True)|(tempDS['user_name'].str.contains(dentPrefix))] # filters for experts only\n",
    "    dataset.reset_index()\n",
    "    \n",
    "\n",
    "# removes banned users\n",
    "fle = open(\"banned.txt\", \"r\")\n",
    "banedUsers = eval(fle.read())\n",
    "for bnd in banedUsers:\n",
    "    dataset = dataset[dataset['user_name'] != bnd]\n",
    "    \n",
    "\n",
    "\n",
    "dataset = dataset[dataset[\"workflow_version\"] > minWorkflowVersion].reset_index(drop=True)# removes pre-beta test data\n",
    "\n",
    "dataset = dataset[dataset[\"annotations\"].str.contains('\"value\":null') == False].reset_index(drop=True) # removes null bounding boxes\n",
    "dataset = dataset[dataset[\"annotations\"].str.contains('\"value\":\\[]') == False].reset_index(drop=True) # removes empty classifications\n",
    "\n",
    "dataset = dataset[dataset.groupby('user_name').user_name.transform('count') >= minClss].reset_index(drop=True) # removes users with less total classifications than the minClss variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creates Directories\n",
    "pathType = pthType+'_Crowdsourced'\n",
    "try:\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType)\n",
    "except:\n",
    "    print('Root Already Created')\n",
    "try:\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\images')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\images\\\\train')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\images\\\\test')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\images\\\\val')\n",
    "except:\n",
    "    print('Images Folder Already Created')\n",
    "try:\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\labels')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\labels\\\\train')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\labels\\\\test')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\labels\\\\val')\n",
    "except:\n",
    "    print('Labels Folder Already Created')\n",
    "try:\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\volunteers')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\volunteers\\\\train')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\volunteers\\\\test')\n",
    "    os.mkdir(saveLoaction+'\\\\'+pathType+'\\\\volunteers\\\\val')\n",
    "except:\n",
    "    print('Volunteers Folder Already Created')\n",
    "\n",
    "\n",
    "if includeIID == True:\n",
    "    pathTypeIID = pthType+'_IID'\n",
    "    try:\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID)\n",
    "    except:\n",
    "        print('IID Root Already Created')\n",
    "    try:\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\images')\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\images\\\\train')\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\images\\\\test')\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\images\\\\val')\n",
    "    except:\n",
    "        print('Images Folder Already Created')\n",
    "    try:\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\labels')\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\labels\\\\train')\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\labels\\\\test')\n",
    "        os.mkdir(saveLoaction+'\\\\'+pathTypeIID+'\\\\labels\\\\val')\n",
    "    except:\n",
    "        print('Labels Folder Already Created')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "indexDone = []\n",
    "#first volunteer for one image\n",
    "for index, row in tqdm(dataset.iterrows()):\n",
    "    width, height, xCord, yCord, labels, IP = [], [], [], [], [], []\n",
    "    if index not in indexDone:\n",
    "        # Removes Unwanted Images\n",
    "        remove = eval(row['subject_data'].replace(\"null\", \"'null'\"))\n",
    "        remove = remove[str(row['subject_ids'])]\n",
    "        remove = remove['Filename'].replace(\".jpg\", \".JPG\")\n",
    "        if remove in imgRemove:\n",
    "            indexDone.append(index)\n",
    "            print('REMOVED UNWANTED IMAGES: ', remove)\n",
    "            continue\n",
    "        temp = dataset.loc[dataset['subject_ids'] == (row['subject_ids'])]\n",
    "        temp = temp.reset_index(drop=True)\n",
    "        numAnno = len(temp)        \n",
    "        # removes annotations and images if there aren't enough annotatiors (see parameters)\n",
    "        if numAnno < minAnnotators:\n",
    "            continue\n",
    "        # all other volunteers for one image\n",
    "        for index2, row2, in temp.iterrows():\n",
    "            \n",
    "            # extracts the username (encrypted IP address)\n",
    "            userIP = row2['user_name']\n",
    "            \n",
    "            # extracts the name of the image\n",
    "            imageName = row2['subject_data']\n",
    "            imageName = imageName.replace(\"null\", \"'null'\")\n",
    "            imageName = eval(imageName)\n",
    "            imageName = imageName[str(row2[\"subject_ids\"])]\n",
    "            imageName = imageName['Filename']\n",
    "            name, extension= os.path.splitext(imageName)\n",
    "            \n",
    "            # fixes file extension for consistency\n",
    "            if extension == '.jpg':\n",
    "                imageName.replace(\".jpg\", \".JPG\")\n",
    "                imageName = (name+'.JPG')\n",
    "            \n",
    "            # extracts the annotation made by the user (encrypted IP address)\n",
    "            annotation = row2['annotations']\n",
    "            annotation = annotation.replace(\"null\", \"'null'\")\n",
    "            annotation = eval(annotation)\n",
    "            annotation = annotation[0][\"value\"]\n",
    "            \n",
    "            #extracts the image width and height\n",
    "            try:\n",
    "                imgSze = row2['metadata']\n",
    "                imgSze = imgSze.replace(\"true\", \"True\")\n",
    "                imgSze = imgSze.replace(\"null\", \"'null'\")\n",
    "                imgSze = eval(imgSze.replace(\"false\", \"False\"))\n",
    "                imgSze = imgSze['subject_dimensions'][0]\n",
    "                originalwidth, originalheight = imgSze['naturalWidth'], imgSze['naturalHeight']\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # traverses the annotation dictionary inside the dataframe to extract all annotations.\n",
    "            widthIID, heightIID, xCordIID, yCordIID, labelsIID = [], [], [], [], []\n",
    "            for ann in annotation:\n",
    "                label = ann[\"details\"]\n",
    "                label = label[0]\n",
    "                # removes unwanted labels and null classes\n",
    "                if label[\"value\"] == 'null' or label[\"value\"] == remveLbl:  \n",
    "                    continue\n",
    "                else:\n",
    "                    # removes label\n",
    "                    if swapLabel:\n",
    "                        label = switchLabel(swapLabel, label)\n",
    "                    # changes class label numbers based on the removed value\n",
    "                    if label[\"value\"] > remveLbl:\n",
    "                        if remveLbl != -1:\n",
    "                            label[\"value\"] -= 1\n",
    "                    \n",
    "                    \n",
    "                    # original values to view all the labels\n",
    "                    labels.append(label[\"value\"])\n",
    "                    width.append(ann[\"width\"])\n",
    "                    height.append(ann[\"height\"])\n",
    "                    xCord.append(ann[\"x\"])\n",
    "                    yCord.append(ann[\"y\"])\n",
    "                    IP.append(userIP)\n",
    "                    if includeIID == True:\n",
    "                        labelsIID.append(label[\"value\"])\n",
    "                        widthIID.append(ann[\"width\"])\n",
    "                        heightIID.append(ann[\"height\"])\n",
    "                        xCordIID.append(ann[\"x\"])\n",
    "                        yCordIID.append(ann[\"y\"])\n",
    "                        IPIID = userIP\n",
    "            if xCordIID:\n",
    "                x2, y2, w2, h2 = PrepForYolo(xCordIID,yCordIID,widthIID,heightIID,originalwidth,originalheight)\n",
    "                x2, y2, w2, h2 = fixOutOfBoundsBoxes(x2, y2, w2, h2)\n",
    "                formats = pd.DataFrame({'label':labelsIID, 'xCord':x2, 'yCord':y2, 'width':w2, 'height':h2})\n",
    "                SaveIID(formats, IPIID, saveLoaction, pathTypeIID, name, imageLocation)\n",
    "                    \n",
    "            # adds compleeted rows to a list to be skpped later on. To avoid repeating labels.\n",
    "            indexDone.append(index2)\n",
    "        if xCord:\n",
    "            x2, y2, w2, h2 = PrepForYolo(xCord,yCord,width,height,originalwidth,originalheight)\n",
    "            x2, y2, w2, h2 = fixOutOfBoundsBoxes(x2, y2, w2, h2)\n",
    "            formats = pd.DataFrame({'label':labels, 'xCord':x2, 'yCord':y2, 'width':w2, 'height':h2})\n",
    "            SaveCrowd(formats, IP, saveLoaction, pathType, name, imageLocation)\n",
    "        \n",
    "        # adds data to a new csv for easy data analysis\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "# removes repeated images in the train dataset that are already in the test dataset\n",
    "removeRepeated(testLocationImg, saveLoaction, pathType)\n",
    "if includeIID == True:\n",
    "    removeRepeated(testLocationImg, saveLoaction, pathTypeIID)\n",
    "\n",
    "# performs train val split, and imports test dataset\n",
    "valSplit(trainValSplit, saveLoaction, pathType, False)\n",
    "if includeTestDS:\n",
    "    injectTestDS(testLocationImg, testLocationLbl, saveLoaction, pathType, False)\n",
    "if includeIID == True:\n",
    "    valSplit(trainValSplit, saveLoaction, pathTypeIID, True)\n",
    "    if includeTestDS:\n",
    "        injectTestDS(testLocationImg, testLocationLbl, saveLoaction, pathTypeIID, True)\n",
    "print(\"############ DONE ############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b92226-4466-4948-9e26-f18cc2a4ea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd6802e-b06a-4ca6-8756-5d4c6e772f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
