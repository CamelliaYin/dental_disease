{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e95df8-f52b-4916-962f-1dbee57e6dab",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "This notebook contains all the experiments conducted for the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163d8c9-08fd-40ab-8e3e-793982519701",
   "metadata": {},
   "source": [
    "## Experiment Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0beb8-b7aa-47f1-b244-8a7fb4b3b3e4",
   "metadata": {},
   "source": [
    "### This experiment set is to test the results using the smaller test dataset of 5 annotators (including 1 expert). Results are reproduced from the development of the crowd yolo model, compared to other models using the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8253cc6-77ab-4c1c-9819-679b473f5f36",
   "metadata": {},
   "source": [
    "The dataset used is a small dataset of 100 radiographs with 5 annotators (500 sets of labels) one of which is compleeted by a dental expert. The expert labels are separated and used as ground truth labels for the test dataset.\n",
    "\n",
    "\n",
    "There are 4 models/techniques to expeiment:\n",
    "- `Crowd Yolo`: A combination of YoloV5 object detection algorithm and a bayesian classifier combination network BCCNet (a network that decides the annotators value as an annotator inorder to train the yolo model). (main model the paper is about)\n",
    "- `Crowd RCNN`: Another crowdsourced object detection model using an RCNN object detection model and a bayesian classifier combination network BCCNet (This main model we are comparing the results to)\n",
    "- `YoloV5 IID`: This model doesn't have crowdsourcing functionality and is just YoloV5 object detection. The data to train this model has to be treated as ground truth data, so a separate dataset is used where different annotators for a single image are separated and treated as individual ground truth labels..\n",
    "- `YoloV5 Agregation Preprocessing`: This algorithm agregates nearby labels of the same disease to create ground truth labels from crowdsourced labels (Not Machine learning). It will also use yoloV5 for the object detection `MIGHT NOT BE NEEDED`\n",
    "\n",
    "\n",
    "`ALL MODELS HAVE HYPERPAMETER TUNING AND ARE TRAINED TO THE BEST OF OUR ABILITY`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f47de0-ab8b-4c58-b113-956638f4eb9e",
   "metadata": {},
   "source": [
    "|  | `Dataset 1 (All Annotators) [CSall]` | `Test Data (Ground Truth) [GTtest]` |\n",
    "|---|---|---|\n",
    "| `Crowd Yolo` | Expert and non-expert Crowdsourced data | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n",
    "| `Crowd  RCNN` | Expert and non-expert Crowdsourced data | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n",
    "| `YoloV5 IID` | Images are repeated and labels are separated to act as ground trouth labels for each radiograph (from same dataset) | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n",
    "| `MIGHT NOT BE NEEDED` `YoloV5 Agregation Preprocessing` | Expert and non-expert Crowdsourced data agregated into ground truth labels | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315ee6b-3bf7-490c-9d27-5f59280518f9",
   "metadata": {},
   "source": [
    "## Experiment Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becaeccc-cdee-4fa8-8aed-1ebdc7cb72e3",
   "metadata": {},
   "source": [
    "There are 3 datasets used in these experiments:\n",
    "- `Dataset 1 (CSall)`: Crowdsourced with both expert and non-expert labels \n",
    "- `Dataset 2 (CSexp)`: Crowdsourced (same dataset as above) with just expert labels filtered, no non-expert labels.  \n",
    "- `Test Dataset (GTtest)`: Expert only ground truth labels with unseen data (small dataset completed by a single expert)\n",
    "\n",
    "There are 4 models/techniques to expeiment:\n",
    "- `Crowd Yolo`: A combination of YoloV5 object detection algorithm and a bayesian classifier combination network BCCNet (a network that decides the annotators value as an annotator inorder to train the yolo model). (main model the paper is about)\n",
    "- `Crowd RCNN`: Another crowdsourced object detection model using an RCNN object detection model and a bayesian classifier combination network BCCNet (This main model we are comparing the results to)\n",
    "- `YoloV5 IID`: This model doesn't have crowdsourcing functionality and is just YoloV5 object detection. The data to train this model has to be treated as ground truth data, so a separate dataset is used where different annotators for a single image are separated and treated as individual ground truth labels..\n",
    "- `YoloV5 Agregation Preprocessing`: This algorithm agregates nearby labels of the same disease to create ground truth labels from crowdsourced labels (Not Machine learning). It will also use yoloV5 for the object detection `MIGHT NOT BE NEEDED`\n",
    "\n",
    "\n",
    "`ALL MODELS HAVE HYPERPAMETER TUNING AND ARE TRAINED TO THE BEST OF OUR ABILITY`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9a1f8-2c21-4857-bdd3-240e4da9d9fa",
   "metadata": {},
   "source": [
    "|  | `Dataset 1 (All Annotators) [CSall]` | `Dataset 2 (Expert Labels Only) [CSexp]` | `Test Data (Ground Truth) [GTtest]` |\n",
    "|---|---|---|---|\n",
    "| `Crowd Yolo` | Expert and non-expert Crowdsourced data | Expert Crowdsourced data | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n",
    "| `Crowd  RCNN` | Expert and non-expert Crowdsourced data | Expert Crowdsourced data | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n",
    "| `YoloV5 IID` | Images are repeated and labels are separated to act as ground trouth labels for each radiograph (from same dataset) | Images are repeated and labels are separated to act as ground trouth labels only for expert annotator labels for each radiograph (from same dataset) | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n",
    "| `MIGHT NOT BE NEEDED` `YoloV5 Agregation Preprocessing` | Expert and non-expert Crowdsourced data agregated into ground truth labels | Expert only Crowdsourced data agregated into ground truth labels | `Expert ground truth unseen data. Used as the test dataset for both train datasets.` |\n",
    "\n",
    "LAST ONE MIGHT NOT BE NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229c0ec-b3ea-4422-ba99-5e573f862ce9",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55803f-149f-4020-bbfc-980fe8391013",
   "metadata": {},
   "source": [
    "#### Please run the 'Install Requirements.ipynb' file in the base folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d0d3c-99d7-41d7-b01f-4dce875167f3",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef439702-e873-4e9b-8a5f-f0b0c71f9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdddetection\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights & Biases  (optional)\n",
    "%pip install -q wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ec26ee-2784-48aa-9f7e-b82d1b047192",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'results_plotter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17824/637207101.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mresults_plotter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_exp_results\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_exp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'results_plotter'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as mpatch\n",
    "import matplotlib\n",
    "from results_plotter import read_exp_results as read_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327df03-53ab-474d-8917-152300d2ede7",
   "metadata": {},
   "source": [
    "### Choice of Hyper Parameters\n",
    "\n",
    "Below are all the inital parameters you can change.\n",
    "\n",
    "\n",
    "\n",
    "| `Parameter` | `Description` | `Type` | `Default` |\n",
    "| --- | --- | --- | --- |\n",
    "| bcc_epoch | start-epoch for BCC+YOLO run; use -1 for no BCC. | int | 0 |\n",
    "| qtfilter_epoch | start-epoch for qt-filter; use -1 for no qt-filter. | int | -1 |\n",
    "| qt_thres_mode | one of '', 'conf-count', 'entropy', 'conf-val' | str | '' |\n",
    "| qt_thres | the threshold value. | float | 0.0 |\n",
    "| hybrid_entropy_thres | the entropy threshold value (only to be used when running the hybrid filter). | float | 0.0 |\n",
    "| hybrid_conf_thres | the confidence threshold value (only to be used when running the hybrid filter). | float | 0.0 |\n",
    "| weights | initial weights path | str | 'yolov5s.pt' |\n",
    "| cfg | model.yaml path | str | '' |\n",
    "| data | dataset.yaml path | str | '../data/coco128.yaml' |\n",
    "| hyp | hyperparameters path | str | '../data/hyps/hyp.scratch.yaml' |\n",
    "| epochs | number of epoch repeates | int | 300 |\n",
    "| batch-size | total batch size for all GPUs | int | 16 |\n",
    "| imgsz | train, val image size (pixels) | int | 640 |\n",
    "| rect | rectangular training | call | store_true |\n",
    "| resume | resume most recent training | bool | False |\n",
    "| nosave | only save final checkpoint | call | store_true |\n",
    "| noval | only validate final epoch | call | store_true |\n",
    "| noautoanchor | disable autoanchor check | call | store_true |\n",
    "| evolve | evolve hyperparameters for x generations | int | const:300 |\n",
    "| bucket | gsutil bucket | str | '' |\n",
    "| cache | --cache images in \"ram\" (default) or \"disk\" | str | const:'ram' |\n",
    "| image-weights | use weighted image selection for training | call | store_true |\n",
    "| device | cuda device, i.e. 0 or 0,1,2,3 or cpu | call | store_true |\n",
    "| multi-scale | vary img-size +/- 50%% | call | store_true |\n",
    "| single-cls | train multi-class data as single-class | call | store_true |\n",
    "| adam | use torch.optim.Adam() optimizer | call | store_true |\n",
    "| sync-bn | use SyncBatchNorm, only available in DDP mode | call | store_true |\n",
    "| workers | maximum number of dataloader workers | int | 8 |\n",
    "| project | save to project/name | N/A | runs/train |\n",
    "| entity | W&B entity | N/A | None |\n",
    "| name | save to project/name | N/A | 'exp' |\n",
    "| exist-ok | existing project/name ok, do not increment | call | store_true |\n",
    "| quad | quad dataloader | call | store_true |\n",
    "| linear-lr | linear LR | call | store_true |\n",
    "| label-smoothing | Label smoothing epsilon | float | 0.0 |\n",
    "| upload_dataset | Upload dataset as W&B artifact table | call | store_true |\n",
    "| bbox_interval | Set bounding-box image logging interval for W&B | int | -1 |\n",
    "| save_period | Log model after every \"save_period\" epoch | int | -1 |\n",
    "| artifact_alias | version of dataset artifact to be used | str | latest |\n",
    "| local_rank | DDP parameter, do not modify | int | -1 |\n",
    "| freeze | Number of layers to freeze. backbone=10, all=24 | int | 0 |\n",
    "| patience | EarlyStopping patience (epochs) | int | 1100 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc76c8e-42ee-402d-9901-c962561abe71",
   "metadata": {},
   "source": [
    "# EXPERIMENT SET 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b8126-83e2-438c-b55a-7a03fb726833",
   "metadata": {},
   "source": [
    "# Experiment 1 (Crowd Yolo Model)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for both dataset using the Crowd Yolo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2589751-3be2-4296-a10a-1b66317e9205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'yolov5_master/requirements.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-utils in c:\\users\\rb01243\\anaconda3\\lib\\site-packages (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r yolov5_master/requirements.txt\n",
    "!pip install python-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e851a6-0d67-4ad0-a288-d3f415d5f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/rb01243/OneDrive - University of Surrey/Documents/GitHub/dental_disease')\n",
    "origDir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ecd43c-73f7-4095-a86c-9ada6aed3837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'C:\\Users\\rb01243\\OneDrive - University of Surrey\\Documents\\GitHub\\src\\train.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#os.chdir(origDir)\n",
    "#os.chdir(os.getcwd()+'/yolov5_master')\n",
    "\n",
    "#%run -i train.py --data ../data/iid-tvt.yaml --batch-size 20 --epochs 18 --weights models/yolov5s.pt\n",
    "#%run -i src/train.py --data data/iid-tvt.yaml --batch-size 20 --epochs 18 --bcc_epoch -1 --cfg src/models/yolov5s.yaml\n",
    "#%run -i yolov5_master/train.py --data data/iid-tvt.yaml --batch-size 20 --epochs 18 --cfg yolov5_master/models/yolov5s.yaml\n",
    "#%run -i train.py --data data/iid-tvt.yaml --batch-size 20 --epochs 18 --weights models/yolov5s.pt --bcc_epoch -1 yolov5_master/train.py yolov5_master/models/yolov5s.pt\n",
    "\n",
    "#os.chdir(origDir) \n",
    "sys.path.append('../src/cyolo_utils')\n",
    "PROJ_PATH = '..'\n",
    "sys.path.append(PROJ_PATH)\n",
    "SRC_PATH = os.path.join(PROJ_PATH, 'src')\n",
    "\n",
    "\n",
    "data = '../data/iidc-tvt.yaml'\n",
    "#data = '../data/cyolo.yaml'\n",
    "batch_size = 20 # Change this to number of train images\n",
    "epochs = 3\n",
    "bcc_epoch = -1 # Involve BCC from epoch number \"bcc_epoch\". Set to -1 for no BCC. 0 for all BCC.\n",
    "exp_num = 10000\n",
    "name = f'exp{exp_num}'\n",
    "\n",
    "!python $SRC_PATH/train.py --data $data \\\n",
    "                           --batch-size $batch_size \\\n",
    "                           --epochs $epochs \\\n",
    "                           --bcc_epoch $bcc_epoch \\\n",
    "                           --name $name \\\n",
    "                           --exist-ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e6f6f7e-266e-4b22-b93e-9b05436c5f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-utils\n",
      "  Downloading python_utils-3.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-utils\n",
      "Successfully installed python-utils-3.1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42c572d3-3bf0-4f14-85e4-3306864bf515",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a7334-bcaa-4460-bc66-86ffcb893705",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ebd1e-c827-4212-99b8-7b7c89eb4fbf",
   "metadata": {},
   "source": [
    "# Experiment 2 (Crowd RCNN)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for both dataset using the Crowd RCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4acab0-a0a4-4db6-a0d1-f30c5cdb7f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e608b-2cb4-4c0f-ae55-64c56438f2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dc3123b-d1df-451d-b577-da40fe0d31fa",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666e3ed-7714-4576-9513-76701ab6e9ab",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ce7d6-a098-4f0a-a72e-9d4a99afe6d3",
   "metadata": {},
   "source": [
    "# Experiment 3 (YoloV5 Random Annotators)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for ranomly selected ground truth labels for both dataset using the YoloV5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52f8cb-982c-4434-a61b-7c47441763c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc2c2e-f4e2-4d9b-86d2-afb1a866bc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db276a6f-4959-4898-893e-9defa8128161",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b14cdd-1de6-41f2-ae6a-bede589625e5",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd9df0-d96d-4939-84a6-567a72a39ecb",
   "metadata": {},
   "source": [
    "# Experiment 4 (YoloV5 Agregation Preprocessing)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for both dataset where ground truth labels are agregated into ground truth labels. This experiment also uses YoloV5 as the object detection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b377a7a-47fa-4bb5-a5e5-1e58aa0f2679",
   "metadata": {},
   "source": [
    "## 4.2  CSexp (Dataset 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65118089-4e0a-4481-9a27-224038669ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1485f-a77a-4d54-adc1-f3fc5558cc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c53a4e0-edf1-4d61-881c-0cfe4d711593",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ee798-6531-4ab1-935d-d7fd70de2e37",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6920cf-b0a9-44dd-bd23-d42ee0cbbbc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee0db88c-d29a-408b-bbef-f53be7847017",
   "metadata": {},
   "source": [
    "# Conclusion Experiment Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820fa65-e57a-4a99-90c9-1a9850c3b9fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5872aa92-309b-400f-b485-f2e4dc00b0e2",
   "metadata": {},
   "source": [
    "# EXPERIMENT SET 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4daa5-6ef8-471d-88ca-38d1a1cf0126",
   "metadata": {},
   "source": [
    "# Experiment 1 (Crowd Yolo Model)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for both dataset using the Crowd Yolo model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f983b-a95a-49f9-a539-157bb1df1a96",
   "metadata": {},
   "source": [
    "## 1.1  CSall (Dataset 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a473999-6457-4943-8cda-eb889f6978ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2022-1-11 torch 1.10.1+cpu CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, obj=1.0, cls=0.5, cls_pw=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mbcc_epoch=0, qtfilter_epoch=-1, qt_thres_mode=, qt_thres=0.0, hybrid_entropy_thres=0.0, hybrid_conf_thres=0.0, weights=yolov5s.pt, cfg=, data=../data/All_Volunteers_Crowdsourced.yaml, hyp=../data/hyps/hyp.scratch.yaml, epochs=20, batch_size=20, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=1100\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\rb0063\\OneDrive - University of Surrey\\Documents\\GitHub\\NewMiccaiRepo\\src\\requirements.txt not found, check failed.\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 270 layers, 7027720 parameters, 7027720 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 344/350 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.00046875\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '..\\data\\datasets\\All_Volunteers_Crowdsourced\\labels\\train' images and labels...2099 found, 0 missing, 0 empty, 35 corrupted: 100%|██████████| 2099/2099 [00:04<00:00, 424.83it/s] \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191202-145329-XIOLG3Y6ROGQ-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191203-081426-XI5K2AEAUAOB-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191203-081540-XCNRBDEEQH4-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191203-163055-XGTHKKVBZOU5-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191213-123126-X+PF6TW1CEZD-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191217-153736-XMX33Z+8LGBB-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191219-152220-XEDN0DZXIDY-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20191223-120752-XLW3BSNMH4OC-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200102-113531-XTHRRWKYATLX-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200107-084427-XVDS5BUKZGPG-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200106-164357-X7VZF7NXIOYZ-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200107-115359-XMUEBMD8V4XC-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200107-143315-XGICHE5RRX+G-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200108-162455-XGV4QYIJNEMV-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200131-152328-X+94+2PH40KD-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200205-100525-XS8STVEHIWX-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200207-104515-XCZPWC7M3M42-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200207-111411-XWKWD+TTNSOG-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200211-123842-XFVO50JPOMSS-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200212-160400-XFDM82Q1MDKA-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200213-091237-XBSLNXLZCS+W-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200214-161200-XGBOETDOVF2P-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200219-162336-XARXYWHFBEKI-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200226-161739-XXHULL2YEFKB-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200226-161910-XZIAXPWRQ9B-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200304-170208-XPSFI6QLIBRL-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200309-120335-XJJSHSIWM0N-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200311-161951-XJR47HUNPQPX-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200313-161128-XXXOTZJDNGYZ-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200317-090812-XSBPZC2KDZFH-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200319-112308-X9TDOT5HQR7E-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200319-112342-XYSKVYI5DTPF-3.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200708-085919-XLW3K+XWUHPO-0.JPG: negative labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20200821-142455-XKG+NAYQKEX+-0.JPG: non-normalized or out of bounds coordinate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ..\\data\\datasets\\All_Volunteers_Crowdsourced\\images\\train\\Unknown-X-20201022-141032-XNLDAG1M+TJB-0.JPG: negative labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ..\\data\\datasets\\All_Volunteers_Crowdsourced\\labels\\train.cache\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'35ba50826e3ac731153a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\OneDrive - University of Surrey\\Documents\\GitHub\\NewMiccaiRepo\\src\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;31m#torch.cuda.empty_cache()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Surrey\\Documents\\GitHub\\NewMiccaiRepo\\src\\train.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevolve\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mWORLD_SIZE\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mRANK\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m             \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Destroying process group... '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_process_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Surrey\\Documents\\GitHub\\NewMiccaiRepo\\src\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(hyp, opt, device, callbacks)\u001b[0m\n\u001b[0;32m    227\u001b[0m                                               prefix=colorstr('train: '))\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbcc_epoch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mfile_volunteers_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_file_volunteers_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvol_id_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvol_id_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[0mn_grid_choices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_anchor_choices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Surrey\\Documents\\GitHub\\NewMiccaiRepo\\src\\cyolo_utils\\train_with_bcc.py\u001b[0m in \u001b[0;36mget_file_volunteers_dict\u001b[1;34m(data_dict, mode, vol_id_map)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mvol_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvol_mode_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvol_file_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mvol_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvol_id_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mfile_vols_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvol_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfile_vols_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Surrey\\Documents\\GitHub\\NewMiccaiRepo\\src\\cyolo_utils\\train_with_bcc.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mvol_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvol_mode_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvol_file_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mvol_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvol_id_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mfile_vols_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvol_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfile_vols_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '35ba50826e3ac731153a'"
     ]
    }
   ],
   "source": [
    "%run -i ../src/train.py --data ../data/All_Volunteers_Crowdsourced.yaml --batch-size 20 --epochs 20 --bcc_epoch 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaafb7-1667-4cf9-aa47-6acf6719a47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4696eaf3-1714-4ef3-98fa-bf8c65819da9",
   "metadata": {},
   "source": [
    "## 1.2  CSexp (Dataset 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de09c0-6529-4395-8a63-7ec7fdc03c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52415a1c-7cbd-4b12-b86b-da2992128118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e63ed56d-d6f7-4a26-afb3-c7ce47721b6c",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c697e63-3378-4e26-92c9-7d724b732729",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b45ce-e606-4285-af72-07574fea9976",
   "metadata": {},
   "source": [
    "# Experiment 2 (Crowd RCNN)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for both dataset using the Crowd RCNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f483542-c475-4af0-839f-fb46f5390657",
   "metadata": {},
   "source": [
    "## 2.1  CSall (Dataset 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c738a97-1f88-4ac8-a31e-f30d1f95d4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661586bb-a974-4827-bbe1-7544554ea747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b418459-5c7a-41fb-ad68-3778917e1c41",
   "metadata": {},
   "source": [
    "## 2.2  CSexp (Dataset 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa25181-ee9b-416b-8196-c11290ce0902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bc5cf-1862-455f-a261-651e8e0a9599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15e06c44-c3ba-4651-a722-4c73897f10a0",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78670b00-6e68-45b2-9c22-f6bc13dc4de9",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11d7f3-7d99-4f76-b240-ac3df05555ac",
   "metadata": {},
   "source": [
    "# Experiment 3 (YoloV5 Random Annotators)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for ranomly selected ground truth labels for both dataset using the YoloV5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3aab15-d2ea-44a9-92be-90f24cc3b6a9",
   "metadata": {},
   "source": [
    "## 3.1  CSall (Dataset 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747a954-0d4f-4ed8-9f8d-c2d13d4a5d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcbb91-a683-42de-8393-8458d455e011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6805404d-be9c-47d3-bfb1-8fe0c8ba3a13",
   "metadata": {},
   "source": [
    "## 3.2  CSexp (Dataset 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2c815-5251-4d88-85ec-60ec8b16d5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ad9ee-b6e7-4a4e-a5e4-a037f4f5e5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8043373-a54a-4eaf-8a2d-96649fe83f6e",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9773f7-612e-449a-9edb-e7bdb5f5c10c",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c10c65-2246-449a-a753-8115207aa3f1",
   "metadata": {},
   "source": [
    "# Experiment 4 (YoloV5 Agregation Preprocessing)\n",
    "\n",
    "### This section runs the experiments, prints results, and plots graphs for both dataset where ground truth labels are agregated into ground truth labels. This experiment also uses YoloV5 as the object detection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4676442-4671-4e5d-a0f5-318c19f04f4b",
   "metadata": {},
   "source": [
    "## 4.1  CSall (Dataset 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e4953-f2e8-45b3-a6f5-5d4f59af554e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cdec6-0eee-4377-ae34-6b7d602b184e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86c244be-7fd6-4a5c-8df0-b74cd05b938e",
   "metadata": {},
   "source": [
    "## 4.2  CSexp (Dataset 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5396a-7972-4cf9-a20b-607d09fd69da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1931c-4fe2-4c11-b5ee-f31d94d93085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2869902-e5fe-4849-815b-d881bdaef5dd",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265babd-7dac-49f1-9953-8d55e276777a",
   "metadata": {},
   "source": [
    "{ analysis goes here }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a9c43-9af8-433e-9e07-56be9e76c5ae",
   "metadata": {},
   "source": [
    "# Conclusion Experiment Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0712c-6551-4721-a6a6-fb9a10b9fe35",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
